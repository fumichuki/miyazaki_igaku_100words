# ğŸš¨ ä¸å…·åˆãƒ¬ãƒãƒ¼ãƒˆ: æ–‡åˆ†å‰²ã¨LLMå‡ºåŠ›ã®æ·±åˆ»ãªå•é¡Œ

## å ±å‘Šæ—¥æ™‚
2026å¹´1æœˆ29æ—¥

## å½±éŸ¿ç¯„å›²
**é‡å¤§åº¦: HIGHï¼ˆã‚·ã‚¹ãƒ†ãƒ ã®ä¸»è¦æ©Ÿèƒ½ãŒæ­£å¸¸å‹•ä½œã—ã¦ã„ãªã„ï¼‰**

---

## ğŸ”´ ä¸å…·åˆ1: æ–‡åˆ†å‰²ãŒéå‰°ã«è¡Œã‚ã‚Œã‚‹ï¼ˆæœ€é‡è¦ï¼‰

### ç¾è±¡
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸè‹±æ–‡ãŒã€æ„å›³ã—ãªã„ç®‡æ‰€ã§åˆ†å‰²ã•ã‚Œã¦6æ–‡ã«ãªã£ã¦ã—ã¾ã†ã€‚

### ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆå®Ÿéš›ã®è‹±æ–‡ï¼‰
```
According to a recent survey.Japan is getting older, and the demand for nursing home are increasing very fast.As the number of elderly people increase, care service cannot keep up, and many facilities are full by people
so a lot of families cannot get in.this situation is a big problem for local communities,and the government must act fast.If there was more staff, the services will be enough.
```

### æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ
- **æ—¥æœ¬èªåŸæ–‡ãŒ3æ–‡**ãªã®ã§ã€**è‹±è¨³ã‚‚3æ–‡**ã«åˆ†å‰²ã•ã‚Œã‚‹ã¹ã
- åŸæ–‡:
  1. æœ€è¿‘ã®èª¿æŸ»ã«ã‚ˆã‚Œã°ã€æ—¥æœ¬ã®é«˜é½¢åŒ–ãŒé€²ã‚€ä¸­ã§ã€ä»‹è­·æ–½è¨­ã®éœ€è¦ãŒæ€¥å¢—ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚
  2. é«˜é½¢è€…äººå£ã®å¢—åŠ ã«ä¼´ã„ã€ä»‹è­·ã‚µãƒ¼ãƒ“ã‚¹ã®æä¾›ãŒè¿½ã„ã¤ã‹ãšã€å¤šãã®æ–½è¨­ãŒæº€å“¡çŠ¶æ…‹ã¨ãªã£ã¦ã„ã‚‹ã€‚
  3. ã“ã®çŠ¶æ³ã¯ã€åœ°åŸŸç¤¾ä¼šã‚„æ”¿åºœã«ã¨ã£ã¦å¤§ããªèª²é¡Œã¨ãªã£ã¦ãŠã‚Šã€è¿…é€Ÿãªå¯¾å¿œãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚

### å®Ÿéš›ã®å‹•ä½œï¼ˆä¸å…·åˆï¼‰
- **6æ–‡ã«éå‰°åˆ†å‰²**ã•ã‚Œã¦ã—ã¾ã†:
  1. `According to a recent survey.`
  2. `Japan is getting older, and the demand for nursing home are increasing very fast.`
  3. `As the number of elderly people increase, care service cannot keep up, and many facilities are full by people.`
  4. `so a lot of families cannot get in.`
  5. `this situation is a big problem for local communities,and the government must act fast.`
  6. `If there was more staff, the services will be enough.`

### æ ¹æœ¬åŸå› 
**å•é¡Œç®‡æ‰€**: `points_normalizer.py` ã® `normalize_user_input()` ãŠã‚ˆã³ `split_into_sentences()`

#### åŸå› 1: ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„ç®‡æ‰€ã§èª¤ã£ã¦åˆ†å‰²
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã€Œsurvey.Japanã€ã€Œfast.Asã€ã€Œin.thisã€ã®ã‚ˆã†ã«ã€**ãƒ”ãƒªã‚ªãƒ‰ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œå¿˜ã‚Œã¦ã„ã‚‹**
- `normalize_user_input()` ã¯ã€ã“ã®ç®‡æ‰€ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã™ã‚‹ï¼ˆæ­£ã—ã„ï¼‰:
  - `survey.Japan` â†’ `survey. Japan`
  - `fast.As` â†’ `fast. As`
  - `in.this` â†’ `in. this`
- **ã—ã‹ã—**ã€`split_into_sentences()` ãŒã€ã“ã®ã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ã‚’ã€Œæ–°ã—ã„æ–‡ã®é–‹å§‹ã€ã¨èª¤èªè­˜ã—ã¦åˆ†å‰²ã—ã¦ã—ã¾ã†

#### åŸå› 2: æ”¹è¡Œã®é€”ä¸­ã§æ–‡ãŒåˆ‡ã‚Œã¦ã„ã‚‹
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«ã¯æ”¹è¡ŒãŒå«ã¾ã‚Œã‚‹:
  ```
  ...full by people
  so a lot of families...
  ```
- æ”¹è¡Œå‰ã«å¥èª­ç‚¹ãŒãªã„ãŸã‚ã€æœ¬æ¥ã¯1æ–‡ã ãŒã€æ”¹è¡Œå¾Œã®ã€Œsoã€ãŒå°æ–‡å­—ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšæ–°ã—ã„æ–‡ã¨ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã‚‹

#### åŸå› 3: å°æ–‡å­—ã§å§‹ã¾ã‚‹æ–‡ã‚’è¨±å®¹ã—ã¦ã„ã‚‹
- `split_into_sentences()` ã¯å°æ–‡å­—å§‹ã¾ã‚Šã®æ–‡ã«å¯¾å¿œã—ã¦ã„ã‚‹ï¼ˆline 157ï¼‰:
  ```python
  parts = re.split(r'([.!?])\s*(?=[A-Za-z0-9"\'\(])', protected)
  ```
- ã“ã‚Œã«ã‚ˆã‚Šã€ã€Œso a lotã€ã€Œthis situationã€ã®ã‚ˆã†ãªå°æ–‡å­—å§‹ã¾ã‚Šã®æ–‡ã‚‚åˆ†å‰²ã•ã‚Œã¦ã—ã¾ã†
- **æ„å›³**: ãƒ¢ãƒã‚¤ãƒ«å…¥åŠ›ã§ã®å°æ–‡å­—å§‹ã¾ã‚Šã«å¯¾å¿œ
- **å‰¯ä½œç”¨**: å®Ÿéš›ã«ã¯ç¶™ç¶šã—ã¦ã„ã‚‹æ–‡ã‚’èª¤ã£ã¦åˆ†å‰²

### å½±éŸ¿
1. **æ–‡ç•ªå·ã®ã‚ºãƒ¬**: 
   - LLMã¯ã€Œ3æ–‡ç›®ã€ã¨ã—ã¦å‡¦ç†ã™ã¹ãå†…å®¹ã‚’ã€å®Ÿéš›ã«ã¯åˆ¥ã®æ–‡ã¨ã—ã¦åˆ¤å®š
   - å‡ºåŠ›ã§ã€Œ3æ–‡ç›®: ï¼ˆé«˜é½¢è€…äººå£ã®å¢—åŠ ã«ä¼´ã„...ï¼‰ã€ã¨è¡¨ç¤ºã•ã‚Œã‚‹ãŒã€å®Ÿéš›ã«ã¯å…¥åŠ›ã®3æ–‡ç›®ã¨ã¯ç•°ãªã‚‹å†…å®¹
   
2. **æ·»å‰Šã®ä¿¡é ¼æ€§ä½ä¸‹**:
   - æ–‡ãŒç´°åˆ‡ã‚Œã«ãªã‚‹ã“ã¨ã§ã€æ–‡è„ˆãŒå¤±ã‚ã‚Œã‚‹
   - é•·ã„æ–‡ã®æ§‹é€ çš„ãªå•é¡Œã‚’æ¤œå‡ºã§ããªã„

---

## ğŸ”´ ä¸å…·åˆ2: LLMãŒæœªæå‡ºã®æ–‡ã«å¯¾ã—ã¦ç„¡é–¢ä¿‚ãªè§£èª¬ã‚’ç”Ÿæˆ

### ç¾è±¡
å‡ºåŠ›ã•ã‚ŒãŸé …ç›®2ã¨3ã§ã€ä»¥ä¸‹ã®å•é¡ŒãŒç™ºç”Ÿ:
```
2
âœ… (æœªæå‡ºï¼šåŸæ–‡ç¬¬2æ–‡)
â†’
âœ… According to a recent survey, as Japan ages, the demand for nursing homes is rapidly increasing

appropriateï¼ˆå½¢å®¹è©ï¼šé©åˆ‡ãªãƒ»ãµã•ã‚ã—ã„ï¼‰ï¼suitableï¼ˆå½¢å®¹è©ï¼šé©ã—ãŸãƒ»å¥½éƒ½åˆãªï¼‰ã§ã€appropriateã¯çŠ¶æ³ã‚„æ–‡è„ˆã«åˆã£ã¦ã„ã‚‹ã“ã¨ã€suitableã¯ç›®çš„ã«åˆã£ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
ã€å‚è€ƒã€‘be appropriate for Aï¼ˆAã«é©åˆ‡ã§ã‚ã‚‹ï¼‰ï¼be suitable for Aï¼ˆAã«é©ã—ã¦ã„ã‚‹ï¼‰
ä¾‹ï¼šThis method is appropriate for beginners. (ã“ã®æ–¹æ³•ã¯åˆå¿ƒè€…ã«é©åˆ‡ã§ã™ã€‚)ï¼This tool is suitable for the task. (ã“ã®é“å…·ã¯ãã®ä½œæ¥­ã«é©ã—ã¦ã„ã¾ã™ã€‚)
```

### å•é¡Œç‚¹
1. **ã€Œæœªæå‡ºã€ã¨è¡¨ç¤ºã—ã¦ã„ã‚‹ã®ã«ã€LLMãŒå‹æ‰‹ã«è‹±æ–‡ã‚’ç”Ÿæˆ**
   - `(æœªæå‡ºï¼šåŸæ–‡ç¬¬2æ–‡)` â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æå‡ºã—ã¦ã„ãªã„
   - ãªã®ã« `According to a recent survey, as Japan ages...` ã¨ã„ã†è‹±æ–‡ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹
   
2. **reasonã®å†…å®¹ãŒå®Œå…¨ã«ç„¡é–¢ä¿‚**
   - ç”Ÿæˆã•ã‚ŒãŸè‹±æ–‡ã«ã¯ã€Œappropriateã€ã‚‚ã€Œsuitableã€ã‚‚å«ã¾ã‚Œã¦ã„ãªã„
   - ã¾ã£ãŸãé–¢ä¿‚ãªã„èªå½™è§£èª¬ï¼ˆappropriate/suitableï¼‰ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹
   - ã“ã‚Œã¯æ˜ã‚‰ã‹ã«LLMã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¹»è¦šï¼‰

3. **æ—¥æœ¬èªè¨³ãŒè¡¨ç¤ºã•ã‚Œãªã„**
   - ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¦ä»¶ã§ã¯ã€ŒNæ–‡ç›®: ï¼ˆæ—¥æœ¬èªè¨³ï¼‰ã€ãŒå¿…é ˆ
   - ã—ã‹ã— `(æœªæå‡ºï¼šåŸæ–‡ç¬¬2æ–‡)` ã¨ã„ã†è¡¨è¨˜ã®ã¿ã§ã€æ—¥æœ¬èªè¨³ãŒæ¬ è½

### æ ¹æœ¬åŸå› 
**å•é¡Œç®‡æ‰€**: `prompts_translation_simple.py` ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ

#### åŸå› 1: ã€Œæœªæå‡ºã€ã®å®šç¾©ãŒæ›–æ˜§
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¯ã€Œæœªæå‡ºã€ã®æ–‡ã«å¯¾ã™ã‚‹æ˜ç¢ºãªæŒ‡ç¤ºãŒãªã„
- LLMãŒã€Œæœªæå‡ºã€ã‚’ã©ã†æ‰±ã†ã¹ãã‹ç†è§£ã—ã¦ã„ãªã„:
  - ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: ãã®æ–‡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ä½•ã‚‚å‡ºåŠ›ã—ãªã„
  - ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: æ¨¡ç¯„è§£ç­”ã‚’ç”Ÿæˆã™ã‚‹
  - ã‚ªãƒ—ã‚·ãƒ§ãƒ³3: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹
  
- ç¾çŠ¶ã€LLMã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³2ï¼ˆæ¨¡ç¯„è§£ç­”ç”Ÿæˆï¼‰ã‚’é¸æŠã—ã¦ã„ã‚‹ãŒã€reasonã®å†…å®¹ãŒç„¡é–¢ä¿‚

#### åŸå› 2: æ–‡åˆ†å‰²ã®ä¸å…·åˆãŒé€£é–çš„ã«å½±éŸ¿
- å‰è¿°ã®ã€Œä¸å…·åˆ1ã€ã«ã‚ˆã‚Šã€å…¥åŠ›ãŒ6æ–‡ã«åˆ†å‰²ã•ã‚Œã‚‹
- ã—ã‹ã—æ—¥æœ¬èªåŸæ–‡ã¯3æ–‡ã®ã¿
- LLMã¯åŸæ–‡ç¬¬2æ–‡ã€ç¬¬3æ–‡ã«å¯¾å¿œã™ã‚‹è‹±æ–‡ãŒãªã„ã¨åˆ¤æ–­ã—ã€Œæœªæå‡ºã€ã¨ãƒãƒ¼ã‚¯ã™ã‚‹ãŒã€å®Ÿéš›ã«ã¯æ–‡åˆ†å‰²ãŒé–“é•ã£ã¦ã„ã‚‹ã ã‘

### å½±éŸ¿
1. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ··ä¹±**:
   - æå‡ºã—ã¦ã„ãªã„æ–‡ã«å¯¾ã—ã¦è§£èª¬ãŒè¡¨ç¤ºã•ã‚Œã‚‹
   - è§£èª¬å†…å®¹ãŒç„¡é–¢ä¿‚ãªãŸã‚ã€ä¿¡é ¼æ€§ãŒå¤±ã‚ã‚Œã‚‹
   
2. **å­¦ç¿’åŠ¹æœã®ä½ä¸‹**:
   - æ­£ã—ã„æ·»å‰Šçµæœã§ãªã„ãŸã‚ã€å­¦ç¿’ã«ä½¿ãˆãªã„

---

## ğŸ”´ ä¸å…·åˆ3: æ—¥æœ¬èªè¨³ã®è¡¨ç¤ºãŒä¸å®Œå…¨

### ç¾è±¡
é …ç›®1ã§ã¯æ­£ã—ãè¡¨ç¤ºã•ã‚Œã‚‹ãŒã€é …ç›®2ã¨3ã§ã¯è¡¨ç¤ºã•ã‚Œãªã„:

**é …ç›®1ï¼ˆæ­£ã—ã„ï¼‰:**
```
3æ–‡ç›®: ï¼ˆé«˜é½¢è€…äººå£ã®å¢—åŠ ã«ä¼´ã„ã€ä»‹è­·ã‚µãƒ¼ãƒ“ã‚¹ã®æä¾›ãŒè¿½ã„ã¤ã‹ãšã€å¤šãã®æ–½è¨­ãŒæº€å“¡çŠ¶æ…‹ã¨ãªã£ã¦ã„ã‚‹ã€‚ï¼‰
```

**é …ç›®2ã¨3ï¼ˆä¸å…·åˆï¼‰:**
```
âœ… (æœªæå‡ºï¼šåŸæ–‡ç¬¬2æ–‡)
âœ… (æœªæå‡ºï¼šåŸæ–‡ç¬¬3æ–‡)
```

### æ ¹æœ¬åŸå› 
ã“ã‚Œã¯**ä¸å…·åˆ2ã®å‰¯ä½œç”¨**:
- ã€Œæœªæå‡ºã€ã®å ´åˆã€æ—¥æœ¬èªè¨³ã‚’è¡¨ç¤ºã™ã‚‹ä»•çµ„ã¿ãŒæ©Ÿèƒ½ã—ã¦ã„ãªã„
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ã€ŒNæ–‡ç›®: ï¼ˆæ—¥æœ¬èªè¨³ï¼‰ã€ãŒå¿…é ˆã¨æŒ‡å®šã—ã¦ã„ã‚‹ãŒã€ã€Œæœªæå‡ºã€ã®å ´åˆã®ä¾‹å¤–å‡¦ç†ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„

---

## ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã®è©³ç´°

### normalize_user_input() ã®å‡¦ç†çµæœ
```
å…¥åŠ›: According to a recent survey.Japan is getting older, and the demand for nursing home are increasing very fast.As the number of elderly people increase, care service cannot keep up, and many facilities are full by people
so a lot of families cannot get in.this situation is a big problem for local communities,and the government must act fast.If there was more staff, the services will be enough.

å‡ºåŠ›: According to a recent survey. Japan is getting older, and the demand for nursing home are increasing very fast. As the number of elderly people increase, care service cannot keep up, and many facilities are full by people. so a lot of families cannot get in. this situation is a big problem for local communities,and the government must act fast. If there was more staff, the services will be enough.
```

**å¤‰æ›´ç‚¹**:
- `survey.Japan` â†’ `survey. Japan` ï¼ˆã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ï¼‰
- `fast.As` â†’ `fast. As` ï¼ˆã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ï¼‰
- `people\nso` â†’ `people. so` ï¼ˆæ”¹è¡Œå¾Œã«ãƒ”ãƒªã‚ªãƒ‰æŒ¿å…¥ï¼‰
- `in.this` â†’ `in. this` ï¼ˆã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ï¼‰
- `fast.If` â†’ `fast. If` ï¼ˆã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥ï¼‰

### split_into_sentences() ã®å‡¦ç†çµæœ
6æ–‡ã«åˆ†å‰²:
1. `According to a recent survey.`
2. `Japan is getting older, and the demand for nursing home are increasing very fast.`
3. `As the number of elderly people increase, care service cannot keep up, and many facilities are full by people.`
4. `so a lot of families cannot get in.`
5. `this situation is a big problem for local communities,and the government must act fast.`
6. `If there was more staff, the services will be enough.`

---

## ğŸ”§ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£æ–¹é‡

### ä¿®æ­£1: æ–‡åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„ï¼ˆæœ€å„ªå…ˆï¼‰

**å•é¡Œ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ„å›³çš„ã«å¥èª­ç‚¹ã‚’çœç•¥ã—ã¦ã„ã‚‹å ´åˆã¨ã€å˜ãªã‚‹ã‚¿ã‚¤ãƒ—ãƒŸã‚¹ã‚’åŒºåˆ¥ã§ãã¦ã„ãªã„

**è§£æ±ºç­–A - ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæ¨å¥¨ï¼‰**:
1. **æ”¹è¡Œã‚’æ–‡ã®åŒºåˆ‡ã‚Šã¨ã—ã¦å„ªå…ˆ**
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ”¹è¡Œã§åŒºåˆ‡ã£ã¦ã„ã‚‹å ´åˆã¯ã€ãã®æ”¹è¡Œã‚’å°Šé‡
   - ãŸã ã—ã€æ”¹è¡Œã®æœ«å°¾ã«å¥èª­ç‚¹ãŒãªã„å ´åˆã¯ã€æ¬¡ã®è¡Œã¨çµåˆ
   
2. **å°æ–‡å­—å§‹ã¾ã‚Šã®æ–‡ã‚’æ–°ã—ã„æ–‡ã¨ã—ã¦æ‰±ã‚ãªã„**
   - ã€Œsoã€ã€Œthisã€ãªã©ã®å°æ–‡å­—å§‹ã¾ã‚Šã¯ã€å‰ã®æ–‡ã®ç¶™ç¶šã¨è¦‹ãªã™
   - ä¾‹å¤–: ãƒ”ãƒªã‚ªãƒ‰ + å¤§æ–‡å­—ã®å¾Œã«å°æ–‡å­—ãŒæ¥ã‚‹å ´åˆã®ã¿åˆ†å‰²
   
3. **ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã®å¤§æ–‡å­—ã®ã¿ã‚’æ–‡ã®é–‹å§‹ã¨è¦‹ãªã™**
   - ç¾åœ¨: `[.!?]\s*(?=[A-Za-z0-9"\'\(])`
   - å¤‰æ›´å¾Œ: `[.!?]\s+(?=[A-Z"\'\(])`ï¼ˆå¤§æ–‡å­—ã®ã¿ã€ã‚¹ãƒšãƒ¼ã‚¹å¿…é ˆï¼‰

**è§£æ±ºç­–B - æ—¥æœ¬èªåŸæ–‡ã®æ–‡æ•°ã«åˆã‚ã›ã‚‹ï¼ˆç©æ¥µçš„ï¼‰**:
1. æ—¥æœ¬èªåŸæ–‡ã®æ–‡æ•°ã‚’å–å¾—ï¼ˆä¾‹: 3æ–‡ï¼‰
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‹±æ–‡ã‚’3æ–‡ã«ãªã‚‹ã‚ˆã†å¼·åˆ¶åˆ†å‰²
3. ãƒ¡ãƒªãƒƒãƒˆ: æ–‡æ•°ã®ã‚ºãƒ¬ãŒç™ºç”Ÿã—ãªã„
4. ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ: å®Ÿè£…ãŒè¤‡é›‘ã€èª¤åˆ†å‰²ã®å¯èƒ½æ€§

### ä¿®æ­£2: ã€Œæœªæå‡ºã€ã®å‡¦ç†ã‚’æ˜ç¢ºåŒ–

**prompts_translation_simple.py** ã«ä»¥ä¸‹ã‚’è¿½åŠ :

```python
### æœªæå‡ºã®æ–‡ã®å‡¦ç†ã€é‡è¦ã€‘

åŸæ–‡ã«ã‚ã‚‹æ–‡ã ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‹±è¨³ã‚’æå‡ºã—ã¦ã„ãªã„å ´åˆ:
- level: "âœ…"
- before: "(æœªæå‡º)"
- after: "(æœªæå‡º)"
- reason: "Næ–‡ç›®: ï¼ˆæœªæå‡ºï¼‰\nï¼ˆæ—¥æœ¬èªè¨³: [åŸæ–‡ã‚’ãã®ã¾ã¾è¨˜è¼‰]ï¼‰\nã“ã®æ–‡ã¯æå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
- æ¨¡ç¯„è§£ç­”ã‚’å‹æ‰‹ã«ç”Ÿæˆã—ãªã„
- ç„¡é–¢ä¿‚ãªèªå½™è§£èª¬ã‚’å‡ºã•ãªã„
```

### ä¿®æ­£3: æ—¥æœ¬èªè¨³ã®å¿…é ˆåŒ–ã‚’å¾¹åº•

**prompts_translation_simple.py** ã®å‡ºåŠ›ä¾‹ã‚’å…¨ã¦è¦‹ç›´ã—:
- âœ…ã®å ´åˆã‚‚å¿…ãšã€ŒNæ–‡ç›®: ï¼ˆæ—¥æœ¬èªè¨³ï¼‰ã€ã‚’å«ã‚ã‚‹
- ã€Œæœªæå‡ºã€ã®å ´åˆã‚‚ã€Œï¼ˆæ—¥æœ¬èªè¨³: ...ï¼‰ã€ã‚’å«ã‚ã‚‹
- LLMãŒçœç•¥ã§ããªã„ã‚ˆã†ã€ä¾‹ã‚’è¿½åŠ 

---

## ğŸ§ª æ¤œè¨¼æ–¹æ³•

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãªã—
```
Input: "I like apples.She likes oranges."
Expected: 2æ–‡ ["I like apples.", "She likes oranges."]
Actual: 2æ–‡ï¼ˆæ­£ã—ã„ï¼‰
```

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: æ”¹è¡Œé€”ä¸­ã§æ–‡ãŒåˆ‡ã‚Œã¦ã„ã‚‹
```
Input: "I like apples\nbut she likes oranges."
Expected: 1æ–‡ ["I like apples but she likes oranges."]
Actual: 2æ–‡ï¼ˆä¸å…·åˆï¼‰
```

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: å°æ–‡å­—å§‹ã¾ã‚Š
```
Input: "I like apples. so does she."
Expected: 1æ–‡ã¾ãŸã¯2æ–‡ï¼ˆæ–‡è„ˆã«ã‚ˆã‚‹ï¼‰
Actual: 2æ–‡ï¼ˆsplit_into_sentencesã¯åˆ†å‰²ã™ã‚‹ï¼‰
```

---

## ğŸ“ é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

### ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«
- `points_normalizer.py` (lines 70-180)
  - `normalize_user_input()`: ãƒ”ãƒªã‚ªãƒ‰å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹æŒ¿å…¥
  - `split_into_sentences()`: æ–‡åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯
  
- `prompts_translation_simple.py` (lines 200-280)
  - reasonãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å®šç¾©
  - å‡ºåŠ›ä¾‹

- `llm_service.py` (lines 150-250)
  - LLMå‘¼ã³å‡ºã—ãƒ­ã‚¸ãƒƒã‚¯
  - æ–‡åˆ†å‰²ã®å®Ÿè¡Œ

### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
- `test_sentence_split_debug.py` ï¼ˆæ–°è¦ä½œæˆï¼‰
  - ä»Šå›ã®ä¸å…·åˆã‚’å†ç¾ã™ã‚‹ãƒ†ã‚¹ãƒˆ
  - å®Ÿè¡Œ: `python test_sentence_split_debug.py`

---

## âš ï¸ å„ªå…ˆåº¦

1. **P0 (æœ€å„ªå…ˆ)**: æ–‡åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ã®ä¿®æ­£
   - å½±éŸ¿ç¯„å›²: å…¨ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
   - ä¿®æ­£ã—ãªã„é™ã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«æ©Ÿèƒ½ã—ãªã„
   
2. **P1 (é«˜å„ªå…ˆ)**: ã€Œæœªæå‡ºã€ã®å‡¦ç†æ˜ç¢ºåŒ–
   - å½±éŸ¿ç¯„å›²: æ–‡æ•°ãŒåˆã‚ãªã„å ´åˆï¼ˆé »ç¹ã«ç™ºç”Ÿï¼‰
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ··ä¹±ã‚’æ‹›ã
   
3. **P2 (ä¸­å„ªå…ˆ)**: æ—¥æœ¬èªè¨³ã®è¡¨ç¤ºå¾¹åº•
   - å½±éŸ¿ç¯„å›²: âœ…ã®é …ç›®
   - å­¦ç¿’åŠ¹æœã«å½±éŸ¿

---

## ğŸ“Œ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… ä¸å…·åˆã‚’å†ç¾ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’ä½œæˆï¼ˆå®Œäº†ï¼‰
2. â³ æ–‡åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ã®ä¿®æ­£æ¡ˆã‚’å®Ÿè£…
3. â³ ã€Œæœªæå‡ºã€å‡¦ç†ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¿½åŠ 
4. â³ å…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§æ¤œè¨¼
5. â³ GitHub ã«ã‚³ãƒŸãƒƒãƒˆ

---

## è£œè¶³: GPTã¸ã®æŠ•ã’æ–¹

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’GPT-4ã«æŠ•ã’ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ·»ä»˜ã—ã¦ãã ã•ã„:

### å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«
```
points_normalizer.py       # æ–‡åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯
constraint_validator.py    # å¥èª­ç‚¹æ­£è¦åŒ–
prompts_translation_simple.py  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
llm_service.py            # LLMå‘¼ã³å‡ºã—
test_sentence_split_debug.py  # ä¸å…·åˆå†ç¾ãƒ†ã‚¹ãƒˆ
```

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹
```
ä»¥ä¸‹ã®ä¸å…·åˆãƒ¬ãƒãƒ¼ãƒˆã¨é–¢é€£ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

ã€ä¸å…·åˆãƒ¬ãƒãƒ¼ãƒˆã€‘
ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è²¼ã‚Šä»˜ã‘ï¼‰

ã€è³ªå•ã€‘
1. æ–‡åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ã®ä¿®æ­£æ¡ˆã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ï¼ˆä¿å®ˆçš„ãƒ»ä¸­é–“ãƒ»ç©æ¥µçš„ï¼‰
2. ã€Œæœªæå‡ºã€ã®å‡¦ç†ã‚’æ˜ç¢ºåŒ–ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¿½åŠ ç®‡æ‰€ã¨å†…å®¹ã‚’ææ¡ˆã—ã¦ãã ã•ã„
3. æ—¥æœ¬èªè¨³ã®è¡¨ç¤ºã‚’å¾¹åº•ã™ã‚‹ãŸã‚ã®ä¿®æ­£æ¡ˆã‚’ææ¡ˆã—ã¦ãã ã•ã„

ã€åˆ¶ç´„æ¡ä»¶ã€‘
- æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã¯å…¨ã¦é€šéã™ã‚‹ã“ã¨
- æ–°ã—ã„ä¸å…·åˆã‚’å°å…¥ã—ãªã„ã“ã¨
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¸ã®å½±éŸ¿ã¯æœ€å°é™ã«ã™ã‚‹ã“ã¨
```

---

**å ±å‘Šè€…**: GitHub Copilot  
**æœ€çµ‚æ›´æ–°**: 2026å¹´1æœˆ29æ—¥


================================================================================
=== CODE FILES
================================================================================


=== points_normalizer.py ===

"""
æ·»å‰Šãƒã‚¤ãƒ³ãƒˆã®æ­£è¦åŒ–å‡¦ç†
- æ–­ç‰‡ã‚’å…¨æ–‡ã«æ‹¡å¼µ
- level ã‚’ âŒ ã¾ãŸã¯ âœ… ã«å¼·åˆ¶
- âœ… ã®å ´åˆã¯ after=before ã«çŸ¯æ­£
- sentence_no ã‚’ä»˜ä¸
"""
import logging
import re
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


# çœç•¥å½¢ãƒªã‚¹ãƒˆï¼ˆãƒ”ãƒªã‚ªãƒ‰ã‚’å«ã‚€ãŒæ–‡æœ«ã§ã¯ãªã„ã‚‚ã®ï¼‰
_ABBREVIATIONS = [
    "a.m.", "p.m.", "e.g.", "i.e.", "etc.",
    "Mr.", "Mrs.", "Ms.", "Dr.", "Prof.",
    "U.S.", "U.K.", "vs.", "vol.", "fig.",
    "Jan.", "Feb.", "Mar.", "Apr.", "Aug.", "Sep.", "Oct.", "Nov.", "Dec.",
    "Mon.", "Tue.", "Wed.", "Thu.", "Fri.", "Sat.", "Sun."
]
_DOT_PLACEHOLDER = "<DOT>"


def _protect_abbreviations(text: str) -> str:
    """
    çœç•¥å½¢ã®ãƒ”ãƒªã‚ªãƒ‰ã‚’ä¸€æ™‚çš„ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ç½®æ›ã—ã¦ä¿è­·ã™ã‚‹
    
    æ–‡æœ«ã®çœç•¥å½¢ï¼ˆä¾‹: "U.S. It"ï¼‰ã®å ´åˆã€çœç•¥å½¢å†…éƒ¨ã®ãƒ”ãƒªã‚ªãƒ‰ã®ã¿ä¿è­·ã—ã€
    æ–‡æœ«ã®ãƒ”ãƒªã‚ªãƒ‰ã¯ä¿è­·ã—ãªã„
    
    Args:
        text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        ä¿è­·ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    protected = text
    
    # çœç•¥å½¢ã‚’ä¿è­·ï¼ˆãŸã ã—ã€æ–‡æœ«åˆ¤å®šã®ãŸã‚ç‰¹åˆ¥ãªå‡¦ç†ãŒå¿…è¦ï¼‰
    for abbr in _ABBREVIATIONS:
        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«ãƒãƒƒãƒãƒ³ã‚°
        # ãŸã ã—ã€çœç•¥å½¢ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ãŒç¶šãå ´åˆã¯æ–‡ã®åŒºåˆ‡ã‚Šã¨è¦‹ãªã™
        # ä¾‹: "U.S. It" ã®å ´åˆã€"U.S." å…¨ä½“ã§ã¯ãªã "U.S" ã®ã¿ä¿è­·
        pattern = re.compile(re.escape(abbr), re.IGNORECASE)
        
        # çœç•¥å½¢ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ãŒç¶šãå ´åˆã¯ã€æœ€å¾Œã®ãƒ”ãƒªã‚ªãƒ‰ä»¥å¤–ã‚’ä¿è­·
        # ä¾‹: "U.S." â†’ "U<DOT>S."
        if abbr.endswith('.'):
            abbr_without_last_dot = abbr[:-1]  # "U.S." â†’ "U.S"
            # "U.S." ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ãŒç¶šãå ´åˆã®ã¿ã€æœ€å¾Œã®ãƒ”ãƒªã‚ªãƒ‰ã‚’æ®‹ã™
            protected = re.sub(
                re.escape(abbr) + r'(?=\s+[A-Z])',
                abbr_without_last_dot.replace(".", _DOT_PLACEHOLDER) + ".",
                protected,
                flags=re.IGNORECASE
            )
            # ãã‚Œä»¥å¤–ã®å ´åˆã¯å…¨ä½“ã‚’ä¿è­·
            protected = pattern.sub(
                lambda m: m.group(0).replace(".", _DOT_PLACEHOLDER),
                protected
            )
    
    # ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«å½¢å¼ï¼ˆA.B.C.ãªã©ï¼‰ã‚’ä¿è­·
    protected = re.sub(r'\b([A-Z])\.(?=\s*[A-Z]\.)', r'\1' + _DOT_PLACEHOLDER, protected)
    
    # å°æ•°ï¼ˆ3.14ãªã©ï¼‰ã‚’ä¿è­·
    protected = re.sub(r'(\d)\.(\d)', r'\1' + _DOT_PLACEHOLDER + r'\2', protected)
    
    return protected


def _restore_abbreviations(text: str) -> str:
    """
    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ãƒ”ãƒªã‚ªãƒ‰ã«æˆ»ã™
    
    Args:
        text: ä¿è­·ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        å¾©å…ƒã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    return text.replace(_DOT_PLACEHOLDER, ".")


def normalize_user_input(text: str) -> str:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’æ­£è¦åŒ–ã™ã‚‹
    
    ä»¥ä¸‹ã®ä¿®æ­£ã‚’è¡Œã†ï¼š
    - ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãªãå¤§æ–‡å­—ãŒç¶šãå ´åˆã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ï¼ˆä¾‹: "word.In" â†’ "word. In"ï¼‰
    - è¤‡æ•°ã®é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
    - æ–‡æœ«ã«ãƒ”ãƒªã‚ªãƒ‰ãŒãªã„å ´åˆã¯è¿½åŠ 
    - æ–‡é ­ã®ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    
    Args:
        text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸè‹±æ–‡
    
    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸè‹±æ–‡
    """
    if not text or not text.strip():
        return ""
    
    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    normalized = text.strip()
    
    # ã‚¹ãƒ†ãƒƒãƒ—0: æ”¹è¡Œã§æ˜ç¢ºã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å ´åˆã€å„è¡Œã«ãƒ”ãƒªã‚ªãƒ‰ã‚’è¿½åŠ 
    # ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ”¹è¡Œã§æ–‡ã‚’åŒºåˆ‡ã£ã¦ã„ã‚‹å ´åˆã«å¯¾å¿œï¼‰
    lines = [line.strip() for line in normalized.split('\n') if line.strip()]
    if len(lines) > 1:
        # å„è¡Œã®æœ«å°¾ã«ãƒ”ãƒªã‚ªãƒ‰ãŒãªã„å ´åˆã¯è¿½åŠ 
        processed_lines = []
        for line in lines:
            if not line.endswith(('.', '?', '!')):
                line = line + '.'
            processed_lines.append(line)
        # æ”¹è¡Œã‚’1ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›ã—ã¦çµåˆ
        normalized = ' '.join(processed_lines)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãªãå¤§æ–‡å­—ã¾ãŸã¯å°æ–‡å­—ãŒç¶šãå ´åˆã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥
    # çœç•¥å½¢ã‚’ä¿è­·ã™ã‚‹å‰ã«å®Ÿè¡Œï¼ˆp.m.In â†’ p.m. Inï¼‰
    # å°æ–‡å­—å§‹ã¾ã‚Šã®æ–‡ã«ã‚‚å¯¾å¿œï¼ˆãƒ¢ãƒã‚¤ãƒ«å…¥åŠ›å¯¾å¿œï¼‰
    normalized = re.sub(r'\.([A-Za-z])', r'. \1', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ç–‘å•ç¬¦ãƒ»æ„Ÿå˜†ç¬¦ã®ç›´å¾Œã‚‚åŒæ§˜ï¼ˆå¤§æ–‡å­—ãƒ»å°æ–‡å­—ä¸¡æ–¹ï¼‰
    normalized = re.sub(r'([?!])([A-Za-z])', r'\1 \2', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: è¤‡æ•°ã®é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: æ–‡æœ«ã«ãƒ”ãƒªã‚ªãƒ‰ãƒ»ç–‘å•ç¬¦ãƒ»æ„Ÿå˜†ç¬¦ãŒãªã„å ´åˆã¯ã€ãƒ”ãƒªã‚ªãƒ‰ã‚’è¿½åŠ 
    if not normalized.endswith(('.', '?', '!')):
        normalized = normalized + '.'
    
    return normalized.strip()


def split_into_sentences(text: str) -> List[str]:
    """
    è‹±æ–‡ã‚’ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã«åˆ†å‰²ã™ã‚‹ï¼ˆçœç•¥å½¢ã«å¯¾å¿œï¼‰
    
    p.m., a.m., e.g., U.S. ãªã©ã®çœç•¥å½¢ã®ãƒ”ãƒªã‚ªãƒ‰ã§åˆ†å‰²ã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹
    ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšåˆ†å‰²å¯èƒ½
    
    Args:
        text: è‹±æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    if not text or not text.strip():
        return []
    
    # ã¾ãšæ”¹è¡Œã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ”¹è¡Œã§æ–‡ã‚’åŒºåˆ‡ã£ã¦ã„ã‚‹å ´åˆï¼‰
    lines = [line.strip() for line in text.strip().split('\n') if line.strip()]
    if len(lines) > 1:
        # æ”¹è¡Œã§æ˜ç¢ºã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’æ¡ç”¨
        return lines
    
    # æ”¹è¡ŒãŒãªã„å ´åˆã¯ã€çœç•¥å½¢ã«å¯¾å¿œã—ãŸåˆ†å‰²ã‚’è¡Œã†
    # æˆ¦ç•¥: çœç•¥å½¢ã® "å†…éƒ¨" ã®ãƒ”ãƒªã‚ªãƒ‰ã®ã¿ä¿è­·ã—ã€æ–‡æœ«ã®ãƒ”ãƒªã‚ªãƒ‰ã¯ä¿è­·ã—ãªã„
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: çœç•¥å½¢ã®ãƒ”ãƒªã‚ªãƒ‰ã‚’ä¿è­·
    protected = _protect_abbreviations(text.strip())
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: æ–‡æœ«å€™è£œã®ãƒ”ãƒªã‚ªãƒ‰ã‚’æ¤œå‡º
    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã§ãªã„ãƒ”ãƒªã‚ªãƒ‰ã¾ãŸã¯ç–‘å•ç¬¦ãƒ»æ„Ÿå˜†ç¬¦ã®å¾Œã«ã€
    # ç©ºç™½ã‚ã‚Š/ãªã—ã§å¤§æ–‡å­—ãƒ»å°æ–‡å­—ãƒ»æ•°å­—ãƒ»å¼•ç”¨ç¬¦ãŒæ¥ã‚‹å ´åˆã¯åˆ†å‰²
    # å°æ–‡å­—å§‹ã¾ã‚Šã®æ–‡ã«ã‚‚å¯¾å¿œï¼ˆãƒ¢ãƒã‚¤ãƒ«å…¥åŠ›å¯¾å¿œï¼‰
    # (?<![<DOT_PLACEHOLDER>]) ã§ä¿è­·ã•ã‚ŒãŸãƒ”ãƒªã‚ªãƒ‰ã®ç›´å¾Œã§ãªã„ã“ã¨ã‚’ç¢ºèª
    parts = re.split(r'([.!?])\s*(?=[A-Za-z0-9"\'\(])', protected)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†å‰²çµæœã‚’æ–‡ã«å†æ§‹æˆ
    sentences = []
    i = 0
    while i < len(parts):
        if i + 1 < len(parts) and parts[i + 1] in '.!?':
            # ãƒ†ã‚­ã‚¹ãƒˆ + å¥èª­ç‚¹ã‚’çµåˆ
            sentence = parts[i] + parts[i + 1]
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å¾©å…ƒ
            restored = _restore_abbreviations(sentence)
            # é‡è¤‡ãƒ”ãƒªã‚ªãƒ‰ã‚’å‰Šé™¤ï¼ˆU.S.. â†’ U.S.ï¼‰
            restored = re.sub(r'\.\.+', '.', restored)
            sentences.append(restored)
            i += 2
        else:
            # æœ€å¾Œã®éƒ¨åˆ†ï¼ˆå¥èª­ç‚¹ãªã—ï¼‰
            if parts[i].strip():
                restored = _restore_abbreviations(parts[i])
                sentences.append(restored)
            i += 1
    
    # ç©ºã®è¦ç´ ã‚’å‰Šé™¤ã—ã€ä¸¡ç«¯ã®ç©ºç™½ã‚’å‰Šé™¤
    sentences = [s.strip() for s in sentences if s.strip()]
    
    return sentences


def find_sentence_containing_fragment(fragment: str, sentences: List[str]) -> tuple:
    """
    æ–­ç‰‡ã‚’å«ã‚€ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’æ¢ã™
    
    Args:
        fragment: æ–­ç‰‡ãƒ†ã‚­ã‚¹ãƒˆ
        sentences: ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
    
    Returns:
        (sentence_index, sentence_text) ã¾ãŸã¯ (None, None)
    """
    fragment_lower = fragment.lower().strip()
    
    for i, sentence in enumerate(sentences):
        if fragment_lower in sentence.lower():
            return (i, sentence)
    
    return (None, None)


def replace_fragment_in_sentence(sentence: str, before_fragment: str, after_fragment: str) -> str:
    """
    ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å†…ã®æ–­ç‰‡ã‚’ç½®æ›ã™ã‚‹ï¼ˆ1å›ã®ã¿ï¼‰
    
    Args:
        sentence: å…ƒã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹
        before_fragment: ç½®æ›å‰ã®æ–­ç‰‡
        after_fragment: ç½®æ›å¾Œã®æ–­ç‰‡
    
    Returns:
        ç½®æ›å¾Œã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹
    """
    # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«1å›ã ã‘ç½®æ›
    pattern = re.compile(re.escape(before_fragment), re.IGNORECASE)
    result = pattern.sub(after_fragment, sentence, count=1)
    return result


def normalize_level(level: str, before: str, after: str) -> tuple:
    """
    level ã‚’ âŒ ã¾ãŸã¯ âœ… ã«æ­£è¦åŒ–ã—ã€after ã‚’èª¿æ•´ã™ã‚‹
    
    ãƒ«ãƒ¼ãƒ«:
    - ğŸ’¡ ãŒå«ã¾ã‚Œã‚‹ â†’ âœ… ã«å¤‰æ›ã—ã€after=before
    - level ãŒç„¡ã„ â†’ âœ… ã«å¤‰æ›ã—ã€after=before
    - âŒ ã®ã¨ãã¯ beforeâ‰ after ã‚’è¨±å¯
    - âœ… ã®ã¨ãã¯ after=before ã«çŸ¯æ­£
    
    Args:
        level: å…ƒã® level
        before: ä¿®æ­£å‰ã®è‹±æ–‡ï¼ˆå…¨æ–‡ï¼‰
        after: ä¿®æ­£å¾Œã®è‹±æ–‡ï¼ˆå…¨æ–‡ï¼‰
    
    Returns:
        (normalized_level, normalized_after)
    """
    # level ãŒç„¡ã„ã€ã¾ãŸã¯ ğŸ’¡ ã‚’å«ã‚€å ´åˆ
    if not level or 'ğŸ’¡' in level:
        logger.info(f"Normalizing level: '{level}' â†’ 'âœ… æ­£ã—ã„è¡¨ç¾' (after=before)")
        return ('âœ… æ­£ã—ã„è¡¨ç¾', before)
    
    # âŒ ã®å ´åˆã¯ãã®ã¾ã¾
    if 'âŒ' in level:
        logger.info(f"Level is âŒ, keeping after: '{after[:50]}...'")
        return (level, after)
    
    # âœ… ã®å ´åˆã¯ after=before ã«çŸ¯æ­£
    if 'âœ…' in level:
        if before != after:
            logger.info(f"Level is âœ… but afterâ‰ before. Setting after=before")
        return (level, before)
    
    # ãã®ä»–ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ âœ…
    logger.info(f"Unknown level '{level}', defaulting to 'âœ… æ­£ã—ã„è¡¨ç¾' (after=before)")
    return ('âœ… æ­£ã—ã„è¡¨ç¾', before)


def normalize_points(
    points: List[Dict[str, Any]],
    normalized_answer: str,
    japanese_sentences: List[str]
) -> List[Dict[str, Any]]:
    """
    points ã‚’æ­£è¦åŒ–ã™ã‚‹
    
    1. before/after ã‚’å…¨æ–‡ã«æ‹¡å¼µ
    2. level ã‚’ âŒ ã¾ãŸã¯ âœ… ã«å¼·åˆ¶
    3. âœ… ã®å ´åˆã¯ after=before ã«çŸ¯æ­£
    4. sentence_no ã‚’ä»˜ä¸
    5. sentence_no æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
    
    Args:
        points: LLMã‹ã‚‰è¿”ã•ã‚ŒãŸ points
        normalized_answer: æ­£è¦åŒ–ã•ã‚ŒãŸå­¦ç”Ÿè‹±æ–‡
        japanese_sentences: æ—¥æœ¬èªåŸæ–‡ã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒªã‚¹ãƒˆ
    
    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸ points
    """
    logger.info(f"Starting points normalization: {len(points)} points")
    
    # å­¦ç”Ÿè‹±æ–‡ã‚’ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã«åˆ†å‰²
    student_sentences = split_into_sentences(normalized_answer)
    logger.info(f"Student answer split into {len(student_sentences)} sentences")
    
    normalized_points = []
    
    for i, point in enumerate(points):
        try:
            original_before = point.get('before', '').strip()
            original_after = point.get('after', '').strip()
            original_level = point.get('level', '')
            
            logger.info(f"Processing point {i+1}: before='{original_before[:50]}...', level='{original_level}'")
            
            # before ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not original_before:
                logger.warning(f"Point {i+1}: Empty before, skipping")
                continue
            
            # æ–­ç‰‡ â†’ å…¨æ–‡ã«æ‹¡å¼µ
            sentence_index, full_sentence = find_sentence_containing_fragment(original_before, student_sentences)
            
            if full_sentence is None:
                # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è­¦å‘Šã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                logger.warning(f"Point {i+1}: Fragment '{original_before[:50]}' not found in student answer, skipping")
                continue
            
            logger.info(f"Point {i+1}: Found in sentence {sentence_index + 1}: '{full_sentence[:50]}...'")
            
            # before ã‚’å…¨æ–‡ã«ç½®æ›
            full_before = full_sentence
            
            # after ã‚’å…¨æ–‡ã«æ‹¡å¼µï¼ˆoriginal_after ãŒæ–­ç‰‡ã®å ´åˆã€ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å†…ã§ç½®æ›ï¼‰
            if 'âŒ' in original_level and original_before != original_after:
                # ä¿®æ­£ãŒå¿…è¦ãªå ´åˆï¼šoriginal_before ã‚’ original_after ã«ç½®æ›
                full_after = replace_fragment_in_sentence(full_sentence, original_before, original_after)
                logger.info(f"Point {i+1}: Replaced fragment in sentence: '{full_after[:50]}...'")
            else:
                # ä¿®æ­£ä¸è¦ãªå ´åˆï¼šafter ã¯ before ã¨åŒã˜
                full_after = full_before
            
            # level ã‚’æ­£è¦åŒ–ã—ã€å¿…è¦ãªã‚‰ after ã‚’èª¿æ•´
            normalized_level, final_after = normalize_level(original_level, full_before, full_after)
            
            # sentence_no ã‚’ä»˜ä¸
            # japanese_sentence ãŒã‚ã‚Œã°ãã‚Œã‚’å…ƒã«ç‰¹å®šã€ãªã‘ã‚Œã° sentence_index+1
            sentence_no = sentence_index + 1
            if point.get('japanese_sentence'):
                # æ—¥æœ¬èªåŸæ–‡ã‹ã‚‰ index ã‚’ç‰¹å®šï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                try:
                    jp_index = japanese_sentences.index(point['japanese_sentence'])
                    sentence_no = jp_index + 1
                    logger.info(f"Point {i+1}: Matched Japanese sentence, sentence_no={sentence_no}")
                except ValueError:
                    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ sentence_index+1 ã‚’ä½¿ç”¨
                    logger.warning(f"Point {i+1}: Japanese sentence not found in original, using sentence_index+1")
            
            # æ­£è¦åŒ–çµæœã‚’è¨­å®š
            point['before'] = full_before
            point['after'] = final_after
            point['level'] = normalized_level
            point['sentence_no'] = sentence_no
            
            normalized_points.append(point)
            logger.info(f"Point {i+1}: Normalized successfully (sentence_no={sentence_no})")
        
        except Exception as e:
            logger.error(f"Error normalizing point {i+1}: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
    
    # sentence_no æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
    normalized_points.sort(key=lambda p: p.get('sentence_no', 9999))
    
    logger.info(f"Points normalization complete: {len(normalized_points)} points")
    return normalized_points


=== constraint_validator.py ===

"""
åˆ¶ç´„æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰æ±ºå®šçš„ãƒã‚§ãƒƒã‚¯
å®®å´å¤§å­¦åŒ»å­¦éƒ¨è‹±ä½œæ–‡ç‰¹è¨“ã‚·ã‚¹ãƒ†ãƒ 

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯LLMã«ä¾å­˜ã›ãšã€ç¢ºå®Ÿã«ä»¥ä¸‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ï¼š
1. èªæ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆ100-120èªã¾ãŸã¯80-120èªï¼‰
2. 2ã¤ã®ç†ç”±/ææ¡ˆ/ä¾‹ã®æ¤œå‡ºï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
"""
import re
import logging
from typing import Dict, Any, List, Tuple

logger = logging.getLogger(__name__)


def normalize_punctuation(text: str) -> str:
    """
    å…¨è§’è¨˜å·ã‚’åŠè§’ã«æ­£è¦åŒ–
    
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¨è§’ã§å…¥åŠ›ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹è¨˜å·ã‚’åŠè§’ã«å¤‰æ›ã—ã¾ã™ï¼š
    - å…¨è§’ãƒ”ãƒªã‚ªãƒ‰ï¼ˆï¼ï¼‰â†’ åŠè§’ãƒ”ãƒªã‚ªãƒ‰ï¼ˆ.ï¼‰
    - å…¨è§’ã‚«ãƒ³ãƒï¼ˆï¼Œï¼‰â†’ åŠè§’ã‚«ãƒ³ãƒï¼ˆ,ï¼‰
    - å…¨è§’ç–‘å•ç¬¦ï¼ˆï¼Ÿï¼‰â†’ åŠè§’ç–‘å•ç¬¦ï¼ˆ?ï¼‰
    - å…¨è§’æ„Ÿå˜†ç¬¦ï¼ˆï¼ï¼‰â†’ åŠè§’æ„Ÿå˜†ç¬¦ï¼ˆ!ï¼‰
    - å…¨è§’ã‚³ãƒ­ãƒ³ï¼ˆï¼šï¼‰â†’ åŠè§’ã‚³ãƒ­ãƒ³ï¼ˆ:ï¼‰
    - å…¨è§’ã‚»ãƒŸã‚³ãƒ­ãƒ³ï¼ˆï¼›ï¼‰â†’ åŠè§’ã‚»ãƒŸã‚³ãƒ­ãƒ³ï¼ˆ;ï¼‰
    - å…¨è§’å¼•ç”¨ç¬¦ï¼ˆ""''ï¼‰â†’ åŠè§’å¼•ç”¨ç¬¦ï¼ˆ""''ï¼‰
    - å…¨è§’ãƒã‚¤ãƒ•ãƒ³ï¼ˆãƒ¼ï¼‰â†’ åŠè§’ãƒã‚¤ãƒ•ãƒ³ï¼ˆ-ï¼‰
    
    Args:
        text: æ­£è¦åŒ–å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not text:
        return text
    
    # å…¨è§’â†’åŠè§’ã®å¤‰æ›ãƒãƒƒãƒ—
    replacements = {
        'ã€‚': '.',  # æ—¥æœ¬èªã®å¥ç‚¹
        'ï¼': '.',  # å…¨è§’ãƒ”ãƒªã‚ªãƒ‰
        'ï¼Œ': ',',
        'ï¼Ÿ': '?',
        'ï¼': '!',
        'ï¼š': ':',
        'ï¼›': ';',
        '"': '"',
        '"': '"',
        ''': "'",
        ''': "'",
        'ï¼ˆ': '(',  # å…¨è§’å·¦æ‹¬å¼§
        'ï¼‰': ')',  # å…¨è§’å³æ‹¬å¼§
        'ã€€': ' ',  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ â†’ åŠè§’ã‚¹ãƒšãƒ¼ã‚¹
        'ãƒ¼': '-',
        'ï¼': '-',
        'â€”': '-',
        'â€“': '-',
    }
    
    result = text
    for full_width, half_width in replacements.items():
        result = result.replace(full_width, half_width)
    
    return result


def deterministic_word_count(text: str) -> int:
    """
    æ±ºå®šçš„ãªèªæ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆLLMéä¾å­˜ï¼‰
    
    ãƒ«ãƒ¼ãƒ«:
    - è‹±å˜èªã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ—¥æœ¬èªã¯é™¤å¤–ï¼‰
    - å¥èª­ç‚¹ã‚’é™¤å¤–
    - ã‚¢ãƒã‚¹ãƒˆãƒ­ãƒ•ã‚£ã‚’å«ã‚€å˜èªï¼ˆdon't, it'sï¼‰ã¯1èªã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    - ãƒã‚¤ãƒ•ãƒ³ã§ã¤ãªãŒã£ãŸå˜èªï¼ˆwell-beingï¼‰ã¯1èªã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    - å…¨è§’è¨˜å·ã¯è‡ªå‹•çš„ã«åŠè§’ã«å¤‰æ›ã•ã‚Œã¾ã™
    
    Args:
        text: ã‚«ã‚¦ãƒ³ãƒˆå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        èªæ•°ï¼ˆæ•´æ•°ï¼‰
    """
    if not text or not text.strip():
        return 0
    
    # å…¨è§’è¨˜å·ã‚’åŠè§’ã«æ­£è¦åŒ–
    text = normalize_punctuation(text)
    
    # è‹±å˜èªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚¢ãƒã‚¹ãƒˆãƒ­ãƒ•ã‚£ã¨ãƒã‚¤ãƒ•ãƒ³ã‚’å«ã‚€ï¼‰
    # \b[\w']+\b ã¯ã‚¢ãƒã‚¹ãƒˆãƒ­ãƒ•ã‚£ã‚’å«ã‚€å˜èªã«ãƒãƒƒãƒ
    # ãƒã‚¤ãƒ•ãƒ³ã§ã¤ãªãŒã£ãŸå˜èªã¯1èªã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
    tokens = re.findall(r"\b[\w'-]+\b", text, re.UNICODE)
    
    # è‹±èªã®å˜èªã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå°‘ãªãã¨ã‚‚1æ–‡å­—ã®è‹±å­—ã‚’å«ã‚€ï¼‰
    english_words = [token for token in tokens if re.search(r'[a-zA-Z]', token)]
    
    count = len(english_words)
    logger.debug(f"Word count: {count} (text length: {len(text)} chars)")
    
    return count


def detect_discourse_markers(text: str) -> List[Tuple[str, int]]:
    """
    ãƒ‡ã‚£ã‚¹ã‚³ãƒ¼ã‚¹ãƒãƒ¼ã‚«ãƒ¼ï¼ˆè«–ç†å±•é–‹ã®æŒ‡æ¨™ï¼‰ã‚’æ¤œå‡º
    
    æ¤œå‡ºå¯¾è±¡:
    - First/Firstly/First of all
    - Second/Secondly
    - Another/Also/Moreover/Furthermore/In addition
    - One reason is/Another reason is
    - For example/For instance (è¤‡æ•°ã®ä¾‹ãŒã‚ã‚‹å ´åˆ)
    
    Args:
        text: æ¤œè¨¼å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        [(ãƒãƒ¼ã‚«ãƒ¼æ–‡å­—åˆ—, ä½ç½®), ...] ã®ãƒªã‚¹ãƒˆ
    """
    # å…¨è§’è¨˜å·ã‚’åŠè§’ã«æ­£è¦åŒ–
    text = normalize_punctuation(text)
    
    markers_patterns = [
        r'\bfirst(?:ly)?\b',
        r'\bfirst of all\b',
        r'\bsecond(?:ly)?\b',
        r'\bthird(?:ly)?\b',
        r'\banother\b',
        r'\balso\b',
        r'\bmoreover\b',
        r'\bfurthermore\b',
        r'\bin addition\b',
        r'\bone reason is\b',
        r'\banother reason is\b',
        r'\bfor example\b',
        r'\bfor instance\b',
        r'\badditionally\b',
        r'\bbesides\b',
    ]
    
    found_markers = []
    
    for pattern in markers_patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            found_markers.append((match.group(), match.start()))
    
    # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
    found_markers.sort(key=lambda x: x[1])
    
    return found_markers


def detect_because_clauses(text: str) -> int:
    """
    ç†ç”±ã‚’ç¤ºã™æ¥ç¶šè©ï¼ˆbecause, since, asï¼‰ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    
    Args:
        text: æ¤œè¨¼å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        ç†ç”±æ¥ç¶šè©ã®æ•°
    """
    # å…¨è§’è¨˜å·ã‚’åŠè§’ã«æ­£è¦åŒ–
    text = normalize_punctuation(text)
    
    patterns = [
        r'\bbecause\b',
        r'\bsince\b',
        r'\bas\b(?!\s+(?:well|usual))',  # "as well", "as usual"ã‚’é™¤å¤–
    ]
    
    count = 0
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        count += len(matches)
    
    return count


def detect_sentence_boundaries(text: str) -> int:
    """
    æ–‡ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆãƒ”ãƒªã‚ªãƒ‰ã€æ„Ÿå˜†ç¬¦ã€ç–‘å•ç¬¦ã§åŒºåˆ‡ã‚‹ï¼‰
    
    å…¨è§’è¨˜å·ï¼ˆï¼ï¼ï¼Ÿï¼‰ã¯è‡ªå‹•çš„ã«åŠè§’ï¼ˆ.!?ï¼‰ã«å¤‰æ›ã•ã‚Œã¾ã™ã€‚
    
    Args:
        text: æ¤œè¨¼å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        æ–‡ã®æ•°
    """
    # å…¨è§’è¨˜å·ã‚’åŠè§’ã«æ­£è¦åŒ–
    text = normalize_punctuation(text)
    
    # Mr., Dr., U.S. ãªã©ã®ç•¥èªã«æ³¨æ„
    # ç°¡æ˜“çš„ãªæ–‡æœ«æ¤œå‡º
    sentences = re.split(r'[.!?]+\s+', text.strip())
    # æœ€å¾Œã®æ–‡ï¼ˆãƒ”ãƒªã‚ªãƒ‰ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„å ´åˆï¼‰ã‚’å«ã‚ã‚‹
    sentences = [s for s in sentences if s.strip()]
    
    return len(sentences)


def detect_two_units(text: str) -> Dict[str, Any]:
    """
    2ã¤ã®ç†ç”±/ææ¡ˆ/ä¾‹ã®å­˜åœ¨ã‚’æ¤œå‡ºï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
    
    æ¤œå‡ºæ–¹æ³•:
    1. ãƒ‡ã‚£ã‚¹ã‚³ãƒ¼ã‚¹ãƒãƒ¼ã‚«ãƒ¼ï¼ˆFirst, Second, Anotherç­‰ï¼‰
    2. ç†ç”±ã‚’ç¤ºã™æ¥ç¶šè©ï¼ˆbecause, sinceï¼‰ã®è¤‡æ•°å›ä½¿ç”¨
    3. æ–‡ã®æ•°ï¼ˆ4æ–‡ä»¥ä¸Šã‚ã‚Œã°2ç†ç”±ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
    
    Args:
        text: æ¤œè¨¼å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        {
            "detected_units": æ¤œå‡ºã•ã‚ŒãŸå˜ä½æ•°ï¼ˆ0-2ï¼‰,
            "has_two_units": 2å˜ä½ä»¥ä¸Šã‹,
            "markers_found": æ¤œå‡ºã•ã‚ŒãŸãƒãƒ¼ã‚«ãƒ¼ã®ãƒªã‚¹ãƒˆ,
            "because_count": ç†ç”±æ¥ç¶šè©ã®æ•°,
            "sentence_count": æ–‡ã®æ•°,
            "confidence": ä¿¡é ¼åº¦ï¼ˆ"high", "medium", "low"ï¼‰,
            "suggestions": æ”¹å–„ææ¡ˆã®ãƒªã‚¹ãƒˆ
        }
    """
    markers = detect_discourse_markers(text)
    because_count = detect_because_clauses(text)
    sentence_count = detect_sentence_boundaries(text)
    
    detected_units = 0
    confidence = "low"
    suggestions = []
    
    # ãƒãƒ¼ã‚«ãƒ¼ãƒ™ãƒ¼ã‚¹ã®æ¤œå‡º
    # First/Second ã®ãƒšã‚¢ãŒã‚ã‚Œã°ç¢ºå®Ÿã«2å˜ä½
    markers_text = [m[0].lower() for m in markers]
    has_first = any('first' in m for m in markers_text)
    has_second = any('second' in m for m in markers_text)
    
    if has_first and has_second:
        detected_units = 2
        confidence = "high"
    elif has_first or has_second:
        detected_units = 1
        confidence = "medium"
        suggestions.append("ç‰‡æ–¹ã®ãƒãƒ¼ã‚«ãƒ¼ï¼ˆFirst/Secondï¼‰ã—ã‹æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€æ–¹ã‚‚æ˜ç¤ºã™ã‚‹ã¨æ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚")
    
    # Another, Alsoç­‰ã®è¿½åŠ ãƒãƒ¼ã‚«ãƒ¼ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    additional_markers = [m for m in markers_text if m in ['another', 'also', 'moreover', 'furthermore', 'in addition', 'additionally']]
    
    if detected_units == 0 and len(additional_markers) >= 1:
        detected_units = 1
        confidence = "medium"
        if len(additional_markers) >= 2:
            detected_units = 2
            confidence = "medium"
    
    # ç†ç”±æ¥ç¶šè©ï¼ˆbecauseç­‰ï¼‰ãŒ2å›ä»¥ä¸Šã‚ã‚Œã°2ç†ç”±ã®å¯èƒ½æ€§
    if detected_units == 0 and because_count >= 2:
        detected_units = 2
        confidence = "medium"
        suggestions.append("ç†ç”±ã‚’ç¤ºã™æ¥ç¶šè©ï¼ˆbecauseç­‰ï¼‰ãŒè¤‡æ•°æ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚First, Second ãªã©ã®ãƒãƒ¼ã‚«ãƒ¼ã‚’ä½¿ã†ã¨ã‚ˆã‚Šæ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚")
    
    # æ–‡ã®æ•°ã§æ¨å®šï¼ˆ4æ–‡ä»¥ä¸Šã‚ã‚Œã°2ç†ç”±ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
    if detected_units == 0 and sentence_count >= 4:
        detected_units = 2
        confidence = "low"
        suggestions.append("4æ–‡ä»¥ä¸Šã‚ã‚Šã¾ã™ãŒã€è«–ç†æ§‹é€ ãŒæ˜ç¢ºã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚First, Second ãªã©ã®ãƒãƒ¼ã‚«ãƒ¼ã‚’ä½¿ã£ã¦ç†ç”±ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚")
    
    # æ¤œå‡ºã§ããªã„å ´åˆã®ææ¡ˆ
    if detected_units == 0:
        suggestions.append("2ã¤ã®ç†ç”±/ææ¡ˆ/ä¾‹ãŒæ˜ç¢ºã«åŒºåˆ¥ã§ãã¾ã›ã‚“ã€‚'First, ...' ã¨ 'Second, ...' ã®ã‚ˆã†ãªæ§‹é€ ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")
    elif detected_units == 1:
        suggestions.append("1ã¤ã®ç†ç”±/ææ¡ˆã®ã¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚‚ã†1ã¤è¿½åŠ ã—ã¦ã€2ã¤ã®ç†ç”±ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚")
    
    return {
        "detected_units": min(detected_units, 2),  # æœ€å¤§2
        "has_two_units": detected_units >= 2,
        "markers_found": [m[0] for m in markers],
        "because_count": because_count,
        "sentence_count": sentence_count,
        "confidence": confidence,
        "suggestions": suggestions
    }


def validate_constraints(
    text: str,
    min_words: int,
    max_words: int,
    required_units: int = 2
) -> Dict[str, Any]:
    """
    ã™ã¹ã¦ã®åˆ¶ç´„ã‚’æ¤œè¨¼ï¼ˆçµ±åˆé–¢æ•°ï¼‰
    
    Args:
        text: æ¤œè¨¼å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        min_words: æœ€å°èªæ•°
        max_words: æœ€å¤§èªæ•°
        required_units: å¿…è¦ãªç†ç”±/ææ¡ˆ/ä¾‹ã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰
        
    Returns:
        {
            "word_count": å®Ÿéš›ã®èªæ•°,
            "within_word_range": èªæ•°ç¯„å›²å†…ã‹,
            "required_units": å¿…è¦ãªå˜ä½æ•°,
            "detected_units": æ¤œå‡ºã•ã‚ŒãŸå˜ä½æ•°,
            "has_required_units": å¿…è¦ãªå˜ä½æ•°ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹,
            "unit_detection_confidence": å˜ä½æ¤œå‡ºã®ä¿¡é ¼åº¦,
            "notes": è©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ,
            "suggestions": æ”¹å–„ææ¡ˆã®ãƒªã‚¹ãƒˆ
        }
    """
    # èªæ•°ã‚«ã‚¦ãƒ³ãƒˆ
    word_count = deterministic_word_count(text)
    within_range = min_words <= word_count <= max_words
    
    # 2å˜ä½æ¤œå‡ºï¼ˆç¿»è¨³å•é¡Œã§ã¯required_units=0ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    if required_units > 0:
        unit_detection = detect_two_units(text)
        detected_units = unit_detection["detected_units"]
        has_required = detected_units >= required_units
    else:
        # ç¿»è¨³å•é¡Œ: å˜ä½æ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—
        unit_detection = {
            "detected_units": 0,
            "confidence": "n/a",
            "markers_found": [],
            "because_count": 0,
            "sentence_count": 0,
            "suggestions": []
        }
        detected_units = 0
        has_required = True  # required_units=0ãªã‚‰å¸¸ã«æº€ãŸã™
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
    notes = []
    suggestions = []
    
    # èªæ•°ãƒã‚§ãƒƒã‚¯
    if not within_range:
        if word_count < min_words:
            shortage = min_words - word_count
            notes.append(f"âš ï¸ èªæ•°ä¸è¶³: {word_count}èªï¼ˆ{shortage}èªä¸è¶³ï¼‰")
            suggestions.append(f"ã‚ã¨{shortage}èªä»¥ä¸Šè¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆç›®æ¨™: {min_words}-{max_words}èªï¼‰")
        else:
            excess = word_count - max_words
            notes.append(f"âš ï¸ èªæ•°è¶…é: {word_count}èªï¼ˆ{excess}èªè¶…éï¼‰")
            suggestions.append(f"{excess}èªä»¥ä¸Šå‰Šæ¸›ã—ã¦ãã ã•ã„ï¼ˆç›®æ¨™: {min_words}-{max_words}èªï¼‰")
    else:
        notes.append(f"âœ… èªæ•°OK: {word_count}èªï¼ˆ{min_words}-{max_words}èªï¼‰")
    
    # å˜ä½æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆrequired_units > 0ã®å ´åˆã®ã¿ï¼‰
    if required_units > 0:
        if not has_required:
            notes.append(f"âš ï¸ ç†ç”±/ææ¡ˆãŒä¸è¶³: {detected_units}å€‹æ¤œå‡ºï¼ˆ{required_units}å€‹å¿…è¦ï¼‰")
            suggestions.extend(unit_detection["suggestions"])
        else:
            confidence_label = {
                "high": "æ˜ç¢º",
                "medium": "ã‚„ã‚„æ˜ç¢º",
                "low": "ä¸æ˜ç¢º"
            }.get(unit_detection["confidence"], "ä¸æ˜")
            notes.append(f"âœ… ç†ç”±/ææ¡ˆOK: {detected_units}å€‹æ¤œå‡ºï¼ˆ{confidence_label}ï¼‰")
            
            # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯æ”¹å–„ææ¡ˆã‚’è¿½åŠ 
            if unit_detection["confidence"] == "low":
                suggestions.append("è«–ç†æ§‹é€ ã‚’ã‚ˆã‚Šæ˜ç¢ºã«ã™ã‚‹ãŸã‚ã€'First, ...' ã¨ 'Second, ...' ã®ã‚ˆã†ãªãƒãƒ¼ã‚«ãƒ¼ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    
    return {
        "word_count": word_count,
        "within_word_range": within_range,
        "required_units": required_units,
        "detected_units": detected_units,
        "has_required_units": has_required,
        "unit_detection_confidence": unit_detection["confidence"],
        "markers_found": unit_detection["markers_found"],
        "because_count": unit_detection["because_count"],
        "sentence_count": unit_detection["sentence_count"],
        "notes": notes,
        "suggestions": suggestions
    }


=== test_sentence_split_debug.py ===

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®æ–‡åˆ†å‰²ä¸å…·åˆã‚’å†ç¾ã™ã‚‹ãƒ†ã‚¹ãƒˆ
"""
from constraint_validator import normalize_punctuation
from points_normalizer import normalize_user_input, split_into_sentences

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ï¼ˆå®Ÿéš›ã®å…¥åŠ›ï¼‰
user_input = """According to a recent survey.Japan is getting older, and the demand for nursing home are increasing very fast.As the number of elderly people increase, care service cannot keep up, and many facilities are full by people
so a lot of families cannot get in.this situation is a big problem for local communities,and the government must act fast.If there was more staff, the services will be enough."""

print("=" * 80)
print("ã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘å…ƒã®å…¥åŠ›:")
print("=" * 80)
print(user_input)
print()

print("=" * 80)
print("ã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘normalize_punctuation å¾Œ:")
print("=" * 80)
normalized_punct = normalize_punctuation(user_input)
print(normalized_punct)
print()

print("=" * 80)
print("ã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘normalize_user_input å¾Œ:")
print("=" * 80)
normalized = normalize_user_input(normalized_punct)
print(normalized)
print()

print("=" * 80)
print("ã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘split_into_sentences å¾Œ:")
print("=" * 80)
sentences = split_into_sentences(normalized)
for i, sent in enumerate(sentences, 1):
    print(f"{i}æ–‡ç›®: {sent}")
print()

print("=" * 80)
print("ã€æœŸå¾…ã•ã‚Œã‚‹æ–‡æ•°ã€‘: 3æ–‡")
print(f"ã€å®Ÿéš›ã®æ–‡æ•°ã€‘: {len(sentences)}æ–‡")
print("=" * 80)

# å•é¡Œç‚¹ã®ç¢ºèª
print("\nã€å•é¡Œç‚¹ã®è©³ç´°åˆ†æã€‘")
print("-" * 80)
print("1. ãƒ”ãƒªã‚ªãƒ‰ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„ç®‡æ‰€:")
issues = [
    "survey.Japan",
    "fast.As", 
    "in.this"
]
for issue in issues:
    if issue in user_input:
        print(f"   âœ— {issue} ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    
print("\n2. æ”¹è¡Œã®é€”ä¸­ã§æ–‡ãŒåˆ‡ã‚Œã¦ã„ã‚‹:")
if "\n" in user_input:
    print("   âœ— æ”¹è¡Œã‚ã‚Šï¼ˆæ”¹è¡Œå‰ã«æ–‡æœ«è¨˜å·ãªã—ï¼‰")
    print(f"   æ”¹è¡Œä½ç½®: '...full by people\\nso a lot...'")

print("\n3. å°æ–‡å­—ã§å§‹ã¾ã‚‹æ–‡:")
lowercase_starts = []
for line in user_input.split("\n"):
    if line and line[0].islower():
        lowercase_starts.append(line[:30] + "...")
        
if lowercase_starts:
    print("   âœ— å°æ–‡å­—å§‹ã¾ã‚Šã‚ã‚Š:")
    for start in lowercase_starts:
        print(f"     - {start}")


=== prompts_translation_simple.py (åˆ¤æ–­é †åºã¨reasonãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆéƒ¨åˆ†) ===

2. å„è‹±æ–‡ã«ã¤ã„ã¦è©•ä¾¡ã™ã‚‹ï¼š
   - åŒã˜æ–‡ã®ç¹°ã‚Šè¿”ã—ã®å ´åˆï¼šæœ€åˆã®1æ–‡ã‚’è©•ä¾¡ã—ã€ã€ŒåŒã˜æ–‡ãŒç¹°ã‚Šè¿”ã•ã‚Œã¦ã„ã¾ã™ã€ã¨æŒ‡æ‘˜
   - æ„å‘³ä¸æ˜ãªæ–‡å­—åˆ—ã®å ´åˆï¼šã€Œè‹±æ–‡ã¨ã—ã¦æ„å‘³ã‚’ãªã—ã¦ã„ã¾ã›ã‚“ã€ã¨æŒ‡æ‘˜
   - é€šå¸¸ã®æ–‡ã®å ´åˆï¼šæ–‡æ³•ãƒ»è¡¨ç¾ã‚’è©•ä¾¡

### åˆ¤æ–­é †åºã€å¿…é ˆã€‘

ä»¥ä¸‹ã®é †ç•ªã§åˆ¤æ–­ã—ã€è©²å½“ã™ã‚‹ã‚‚ã®ãŒã‚ã‚Œã°ãã“ã§åˆ¤å®šã‚’ç¢ºå®šã™ã‚‹ï¼š

1. **ğŸš¨ğŸš¨ğŸš¨ æ˜ç¢ºãªæ–‡æ³•ãƒŸã‚¹ï¼ˆæœ€å„ªå…ˆãƒ»å¿…ãšæ¤œå‡ºï¼‰ğŸš¨ğŸš¨ğŸš¨** â†’ âŒæ–‡æ³•ãƒŸã‚¹
   
   **ã€ä¸»èªã¨å‹•è©ã®ä¸ä¸€è‡´ã€‘** - çµ¶å¯¾ã«âŒã«ã™ã‚‹ã“ã¨ï¼ˆæœ€é‡è¦ï¼‰ï¼š
   - å˜æ•°ä¸»èª + è¤‡æ•°å‹•è©ï¼š
     âŒ "This habit are" â†’ "This habit is"
     âŒ "This new habit are" â†’ "This new habit is"
     âŒ "The method work" â†’ "The method works"
     âŒ "My sister like" â†’ "My sister likes"
     âŒ "He don't" â†’ "He doesn't"
   
   - è¤‡æ•°ä¸»èª + å˜æ•°å‹•è©ï¼š
     âŒ "These habits is" â†’ "These habits are"
     âŒ "The students likes" â†’ "The students like"
     âŒ "They doesn't" â†’ "They don't"
   
   **ã€ãƒã‚§ãƒƒã‚¯æ–¹æ³•ã€‘æ–‡æ³•ãƒŸã‚¹ã‚’è¦‹é€ƒã•ãªã„ãŸã‚ã«ï¼š**
   - ä¸»èªã‚’ç‰¹å®šï¼ˆthis/that/a/the + å˜æ•°åè© â†’ å˜æ•°ã€these/those/è¤‡æ•°åè© â†’ è¤‡æ•°ï¼‰
   - å‹•è©ã®å½¢ã‚’ç¢ºèªï¼ˆis/are, -sä»˜ã/ãªã—ã€does/doï¼‰
   - ä¸€è‡´ã—ãªã„å ´åˆã¯**å¿…ãšâŒ**ï¼ˆbeforeã¨afterãŒåŒä¸€ã§ã‚‚âŒï¼‰
   
   **ã€ä¸å®Œå…¨ãªæ–‡ã€‘** - çµ¶å¯¾ã«âŒã«ã™ã‚‹ã“ã¨ï¼š
   - æ–‡æœ«ãŒæ¥ç¶šè©ã§çµ‚ã‚ã£ã¦ã„ã‚‹ï¼š
     âŒ "I like apples and" â†’ ã€Œandã€ã®å¾Œã«æ–‡ãŒç¶šã‹ãªã„
     âŒ "eight hours and Afterward" â†’ ã€Œandã€ã®å¾Œã«æ–‡ãŒç¶šãã¹ã
   - æ–‡æœ«ãŒå‰ç½®è©ã§çµ‚ã‚ã£ã¦ã„ã‚‹ï¼ˆå½¢å¼çš„ãªå ´åˆã‚’é™¤ãï¼‰
   
   **ã€å¥èª­ç‚¹ã®èª¤ã‚Šã€‘** - çµ¶å¯¾ã«âŒã«ã™ã‚‹ã“ã¨ï¼š
   - ãƒ”ãƒªã‚ªãƒ‰ã®å¾Œã«å°æ–‡å­—ã®æ¥ç¶šè©ï¼š
     âŒ "highest scores. while the group" â†’ ãƒ”ãƒªã‚ªãƒ‰ã®å¾Œã«å°æ–‡å­—ã®ã€Œwhileã€
     âŒ "important point. and then" â†’ ãƒ”ãƒªã‚ªãƒ‰ã®å¾Œã«å°æ–‡å­—ã®ã€Œandã€
   - ã‚«ãƒ³ãƒã®ä»£ã‚ã‚Šã«ãƒ”ãƒªã‚ªãƒ‰ï¼š
     âŒ "As a trial. I started" â†’ ãƒ”ãƒªã‚ªãƒ‰ã®ä»£ã‚ã‚Šã«ã‚«ãƒ³ãƒãŒå¿…è¦
   
   **ã€æ™‚åˆ¶ã®èª¤ã‚Šã€‘** - çµ¶å¯¾ã«âŒã«ã™ã‚‹ã“ã¨ï¼š
   - "Yesterday, I go" â†’ "Yesterday, I went"
   
   **ã€å† è©ã®æ˜ç¢ºãªèª¤ã‚Šã€‘** - çµ¶å¯¾ã«âŒã«ã™ã‚‹ã“ã¨ï¼š
   - "I am student" â†’ "I am a student"

2. **beforeã¨afterãŒå®Œå…¨ã«åŒä¸€ã‹ãƒã‚§ãƒƒã‚¯**
   - åŒä¸€ãªã‚‰ â†’ âœ…æ­£ã—ã„è¡¨ç¾
   - ç•°ãªã‚‹ãªã‚‰ â†’ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸

3. **æ—¥æœ¬èªåŸæ–‡ã®å†…å®¹ãŒæ­£ç¢ºã«ä¼ã‚ã£ã¦ã„ãªã„** â†’ âŒæ„å‘³ä¸ä¸€è‡´
   ä¾‹ï¼š
   - æ—¥æœ¬èªã€Œæ¯æ—¥ã€â†’ è‹±èªã€Œsometimesã€ï¼ˆé »åº¦ãŒé•ã†ï¼‰
   - æ—¥æœ¬èªã€Œå¢—åŠ ã—ãŸã€â†’ è‹±èªã€Œdecreasedã€ï¼ˆé€†ã®æ„å‘³ï¼‰
   - æ—¥æœ¬èªã€Œ3ã¤ã®ç†ç”±ã€â†’ è‹±èªã€Œtwo reasonsã€ï¼ˆæ•°ãŒé•ã†ï¼‰

4. **æ˜ç¢ºãªã‚³ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒŸã‚¹** â†’ âŒèª¤ç”¨
   ä¾‹ï¼š
   - "make a mistake" ã‚’ "do a mistake" ã¨æ›¸ã„ãŸå ´åˆ
   - "take a shower" ã‚’ "make a shower" ã¨æ›¸ã„ãŸå ´åˆ

5. **ãã‚Œä»¥å¤–ï¼ˆbeforeã¨afterãŒç•°ãªã‚‹ãŒæ–‡æ³•ãƒŸã‚¹ã§ã¯ãªã„ï¼‰** â†’ âœ…æ­£ã—ã„è¡¨ç¾ã¨ã—ã¦æ‰±ã†
   - **é‡è¦ï¼šã“ã®å ´åˆã€afterã‚’beforeã¨åŒä¸€ã«ã™ã‚‹ã“ã¨**
   - èªå½™é¸æŠã®å¥½ã¿ã€æ§‹æ–‡ã®é¸æŠè‚¢ã€æ–‡ä½“ã®é•ã„ â†’ å…¨ã¦âœ…
   - beforeãŒæ­£ã—ã‘ã‚Œã°ã€afterã‚‚åŒã˜å†…å®¹ã«ã™ã‚‹

**ğŸš¨ğŸš¨ğŸš¨çµ¶å¯¾å³å®ˆğŸš¨ğŸš¨ğŸš¨**
- **âœ…æ­£ã—ã„è¡¨ç¾ã®å ´åˆã€beforeã¨afterã¯å®Œå…¨ã«åŒä¸€ã«ã™ã‚‹ã“ã¨**
- **èªå½™ã‚„æ§‹æ–‡ã®å¥½ã¿ã®é•ã„ã¯ã€å…¨ã¦âœ…ã¨ã—ã¦æ‰±ã†**
- **ğŸ’¡æ”¹å–„ææ¡ˆã¯çµ¶å¯¾ã«å‡ºã•ãªã„**

### ã€å‡ºåŠ›ãƒ©ãƒ™ãƒ«ã¯2ç¨®é¡ã®ã¿ - ğŸ’¡æ”¹å–„ææ¡ˆã¯å®Œå…¨å»ƒæ­¢ã€‘

- **âœ… æ­£ã—ã„è¡¨ç¾**: æ–‡æ³•çš„ã«æ­£ã—ã„ã€æ„å‘³ãŒé€šã‚‹ â†’ beforeã¨afterã¯å®Œå…¨ã«åŒä¸€
- **âŒ æ–‡æ³•ãƒŸã‚¹ / æ„å‘³ä¸ä¸€è‡´ / èª¤ç”¨**: æ¸›ç‚¹ãƒ¬ãƒ™ãƒ«ã®æ˜ç¢ºãªèª¤ã‚Šã®ã¿

ğŸš¨ğŸš¨ğŸš¨ã€çµ¶å¯¾å³å®ˆã€‘ğŸ’¡æ”¹å–„ææ¡ˆã¯ä¸€åˆ‡å‡ºã•ãªã„ï¼ğŸš¨ğŸš¨ğŸš¨
- ã€Œã‚ˆã‚Šè‰¯ã„è¡¨ç¾ãŒã‚ã‚‹ã€ã¯ âœ… ã¨ã—ã¦æ‰±ã†ï¼ˆafter=beforeï¼‰
- å¥½ã¿ã‚„æ–‡ä½“ã®é•ã„ã¯ âœ… ã¨ã—ã¦æ‰±ã†ï¼ˆafter=beforeï¼‰
- æ¸›ç‚¹ã•ã‚Œãªã„è¡¨ç¾ã¯å…¨ã¦ âœ…

### level_typeã®åˆ†é¡ï¼ˆâŒã‚’å‡ºã™å ´åˆã®ã¿å¿…é ˆï¼‰

âŒã‚’å‡ºã™å ´åˆã¯ã€ä»¥ä¸‹ã‹ã‚‰é¸ã¶ï¼š
- "grammar_error" : æ–‡æ³•ã®èª¤ã‚Š
- "semantic_mismatch" : æ„å‘³ã®ä¸ä¸€è‡´
- "collocation_error" : ã‚³ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒŸã‚¹

### reasonã®è¨˜è¿°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€å¿…é ˆãƒ»å³å®ˆã€‘- kagoshimaé¢¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

ğŸš¨ğŸš¨ğŸš¨ã€å‡ºåŠ›ä¾‹ã‚’å¿…ãšå‚è€ƒã«ã—ã¦ãã ã•ã„ - âŒã§ã‚‚âœ…ã§ã‚‚åŒã˜å½¢å¼ã€‘ğŸš¨ğŸš¨ğŸš¨

**å‡ºåŠ›ä¾‹1ï¼ˆâœ…ã®å ´åˆ - æ­£ã—ã„è¡¨ç¾ã®èªå½™è§£èª¬ï¼‰:**
```
"reason": "1æ–‡ç›®: The project aims to reduce traffic jams in cities by using AI technology.\\nï¼ˆã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€AIæŠ€è¡“ã‚’ä½¿ã£ã¦éƒ½å¸‚ã®äº¤é€šæ¸‹æ»ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚ï¼‰\\nreduceï¼ˆå‹•è©ï¼šæ¸›ã‚‰ã™ï¼šå¯¾è±¡ãŒå…·ä½“çš„ãªé‡ï¼‰ï¼alleviateï¼ˆå‹•è©ï¼šç·©å’Œã™ã‚‹ï¼šå¯¾è±¡ãŒæŠ½è±¡çš„ãªäº‹è±¡ï¼‰ã§ã€reduceã¯å…·ä½“çš„ãªæ•°å€¤ã®æ¸›å°‘ã«ä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šãã€alleviateã¯å•é¡Œã‚„è² æ‹…ã®ç·©å’Œã«ä½¿ã‚ã‚Œã‚‹ã€‚\\nã€å‚è€ƒã€‘reduce trafficï¼ˆäº¤é€šé‡ã‚’æ¸›ã‚‰ã™ï¼‰ï¼alleviate congestionï¼ˆæ¸‹æ»ã‚’ç·©å’Œã™ã‚‹ï¼‰\\nä¾‹ï¼šWe need to reduce costs. (ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚)ï¼Measures were taken to alleviate the pain. (ç—›ã¿ã‚’å’Œã‚‰ã’ã‚‹ãŸã‚ã®æªç½®ãŒå–ã‚‰ã‚ŒãŸã€‚)"
```

**å‡ºåŠ›ä¾‹2ï¼ˆâŒã®å ´åˆ - æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ç†ç”±ï¼‰:**
```
"reason": "2æ–‡ç›®: In particular, a system was introduced that analyzes real-time traffic data and optimize the timing of traffic signals.\\nï¼ˆç‰¹ã«ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®äº¤é€šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ä¿¡å·ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æœ€é©åŒ–ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãŒå°å…¥ã•ã‚ŒãŸã€‚ï¼‰\\noptimizeï¼ˆå‹•è©ï¼šæœ€é©åŒ–ã™ã‚‹ï¼šãƒ—ãƒ­ã‚»ã‚¹ã‚„æ©Ÿèƒ½ï¼‰ï¼improveï¼ˆå‹•è©ï¼šæ”¹å–„ã™ã‚‹ï¼šå…¨èˆ¬çš„ã«ï¼‰ã§ã€optimizeã¯ç‰¹å®šã®æ¡ä»¶ä¸‹ã§ã®æœ€è‰¯åŒ–ã«ä½¿ã‚ã‚Œã€improveã¯å…¨ä½“çš„ãªå‘ä¸Šã«ä½¿ã‚ã‚Œã‚‹ã€‚æ–‡æ³•ã‚¨ãƒ©ãƒ¼ï¼š'analyzes'ã¨ä¸¦åˆ—ã®'optimize'ã¯'optimizes'ã«ã™ã¹ãã€‚\\nã€å‚è€ƒã€‘optimize performanceï¼ˆæ€§èƒ½ã‚’æœ€é©åŒ–ã™ã‚‹ï¼‰ï¼improve qualityï¼ˆå“è³ªã‚’æ”¹å–„ã™ã‚‹ï¼‰\\nä¾‹ï¼šWe aim to optimize the system's efficiency. (ã‚·ã‚¹ãƒ†ãƒ ã®åŠ¹ç‡ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚)ï¼Efforts were made to improve the service. (ã‚µãƒ¼ãƒ“ã‚¹ã®æ”¹å–„ã«åŠªã‚ãŸã€‚)"
```

**å‡ºåŠ›ä¾‹3ï¼ˆa number of / the number of - âŒã®å ´åˆï¼‰:**
```
"reason": "1æ–‡ç›®: A number of students submitted the form online.\\nï¼ˆå¤šãã®å­¦ç”ŸãŒãã®ç”¨ç´™ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§æå‡ºã—ãŸã€‚ï¼‰\\na number ofï¼ˆåè©å¥ï¼šå¤šãã®ï½ï¼‰ï¼the number ofï¼ˆåè©å¥ï¼šï½ã®æ•°ï¼‰ã§ã€a number of ã¯ã€ŒãŸãã•ã‚“ã€ã¨ã„ã†é‡ã€the number of ã¯ã€Œæ•°ãã®ã‚‚ã®ã€ã¨ã„ã†æ•°é‡ã‚’è¡¨ã—ã¾ã™ã€‚a number of ã¯å¾Œã‚ãŒè¤‡æ•°åè©ãªã®ã§å‹•è©ã‚‚è¤‡æ•°ã«ãªã‚Šã‚„ã™ã„ï¼ˆA number of students are â€¦ï¼‰ã€‚the number of ã¯ã€Œæ•°ã€ãŒä¸»èªãªã®ã§å˜æ•°æ‰±ã„ï¼ˆThe number of students is â€¦ï¼‰ã€‚\\nã€å‚è€ƒã€‘a number of + è¤‡æ•°åè©ï¼ˆå¤šãã®ï½ï¼‰ï¼the number of + è¤‡æ•°åè©ï¼ˆï½ã®æ•°ï¼‰\\nä¾‹ï¼šA number of people were absent. (å¤šãã®äººãŒæ¬ å¸­ã—ãŸã€‚)ï¼The number of people was increasing. (äººã®æ•°ãŒå¢—ãˆã¦ã„ãŸã€‚)"
```

**å‡ºåŠ›ä¾‹4ï¼ˆä¸»èªå‹•è©ä¸ä¸€è‡´ - âŒã®å ´åˆï¼‰:**
```
"reason": "1æ–‡ç›®: This new habit are very effective to reduce my stress.\\nï¼ˆã“ã®æ–°ã—ã„ç¿’æ…£ã¯ã€ç§ã®ã‚¹ãƒˆãƒ¬ã‚¹ã‚’æ¸›ã‚‰ã™ã®ã«éå¸¸ã«åŠ¹æœçš„ã ã€‚ï¼‰\\nareï¼ˆå‹•è©ï¼šï½ã§ã‚ã‚‹ï¼šè¤‡æ•°å½¢ï¼‰ï¼isï¼ˆå‹•è©ï¼šï½ã§ã‚ã‚‹ï¼šå˜æ•°å½¢ï¼‰ã§ã€ä¸»èªãŒå˜æ•°å½¢ã€ŒThis new habitã€ã®å ´åˆã€å‹•è©ã¯å˜æ•°å½¢ã®ã€Œisã€ã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹ã€‚\\nã€å‚è€ƒã€‘This habit is â€¦ï¼ˆã“ã®ç¿’æ…£ã¯ï½ã ï¼‰ï¼These habits are â€¦ï¼ˆã“ã‚Œã‚‰ã®ç¿’æ…£ã¯ï½ã ï¼‰\\nä¾‹ï¼šThis method is effective. (ã“ã®æ–¹æ³•ã¯åŠ¹æœçš„ã ã€‚)ï¼These methods are effective. (ã“ã‚Œã‚‰ã®æ–¹æ³•ã¯åŠ¹æœçš„ã ã€‚)"
```

ğŸš¨ğŸš¨ğŸš¨ã€ä¸Šè¨˜ã®å‡ºåŠ›ä¾‹ã‚’å¿…ãšå‚è€ƒã«ã™ã‚‹ã“ã¨ã€‘ğŸš¨ğŸš¨ğŸš¨
- ã™ã¹ã¦ã®ä¾‹ã§ã€ŒNæ–‡ç›®: ã€ã¨ã€Œï¼ˆæ—¥æœ¬èªè¨³ï¼‰ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹
- âŒã§ã‚‚âœ…ã§ã‚‚åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

---

**å¿…é ˆå½¢å¼ï¼ˆkagoshimaé¢¨ãƒ»5ç‚¹ã‚»ãƒƒãƒˆï¼‰- âŒ/âœ…å…±é€š:**
```
Næ–‡ç›®: [è‹±æ–‡ãã®ã¾ã¾]
ï¼ˆ[æ—¥æœ¬èªè¨³]ï¼‰
[èªA]ï¼ˆå“è©ï¼šæ„å‘³ï¼šæ–‡è„ˆï¼‰ï¼[èªB]ï¼ˆå“è©ï¼šæ„å‘³ï¼šæ–‡è„ˆï¼‰ã§ã€[é•ã„ã®è©³ç´°èª¬æ˜]ã€‚
ã€å‚è€ƒã€‘[æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³A]ï¼[æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³B]
ä¾‹ï¼š[ä¾‹æ–‡1] ([å’Œè¨³1])ï¼[ä¾‹æ–‡2] ([å’Œè¨³2])
```

**ğŸš¨é‡è¦ğŸš¨ ã“ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ level ãŒ âŒ ã§ã‚‚ âœ… ã§ã‚‚å¿…ãšé©ç”¨ã™ã‚‹**
- âŒã®å ´åˆ: èª¤ã‚Šã®ä¿®æ­£ç†ç”±ã‚’èª¬æ˜
- âœ…ã®å ´åˆ: æ­£ã—ã„è¡¨ç¾ã®èªå½™è§£èª¬ã‚’æä¾›
- **ã©ã¡ã‚‰ã®å ´åˆã‚‚ã€ŒNæ–‡ç›®: ã€ã¨ã€Œï¼ˆæ—¥æœ¬èªè¨³ï¼‰ã€ã¯çœç•¥ç¦æ­¢**

**çµ¶å¯¾å³å®ˆ:**
1. **âŒã§ã‚‚âœ…ã§ã‚‚ã€å¿…ãšã€ŒNæ–‡ç›®: è‹±æ–‡å…¨æ–‡ã€ã§å§‹ã‚ã‚‹**
2. **âŒã§ã‚‚âœ…ã§ã‚‚ã€å¿…ãšæ¬¡ã®è¡Œã«ã€Œï¼ˆæ—¥æœ¬èªè¨³ï¼‰ã€ã‚’è¨˜è¼‰**
3. èªå½™æ¯”è¼ƒã¯å¿…ãšAï¼Bå½¢å¼ã§2ã¤ä»¥ä¸Š
4. ã€å‚è€ƒã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆï¼ˆæ–‡æ³•ãƒ»èªæ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æç¤ºï¼‰
5. ä¾‹æ–‡ã¯å¿…ãš2ã¤ã€å’Œè¨³ã¯ä¸¸æ‹¬å¼§()ã§å›²ã‚€ï¼ˆæ—¥æœ¬èªå¼•ç”¨ç¬¦ã€Œã€ã¯ä½¿ã‚ãªã„ï¼‰
6. ä¾‹æ–‡ã¯å­¦ç”Ÿã®è‹±æ–‡ã¨ç•°ãªã‚‹æ–°ã—ã„ä¾‹ã‚’æç¤ºï¼ˆåŒä¸€æ–‡ã®ç¹°ã‚Šè¿”ã—ç¦æ­¢ï¼‰

**å‡ºåŠ›ä¾‹2ï¼ˆbe likely to / It is likely thatï¼‰:**
```
"reason": "2æ–‡ç›®: She is likely to arrive late because of the traffic.\\nï¼ˆäº¤é€šã®ã›ã„ã§ã€å½¼å¥³ã¯é…ã‚Œã¦åˆ°ç€ã—ãã†ã ã€‚ï¼‰\\nlikelyï¼ˆå½¢å®¹è©ï¼šï½ã—ãã†ã ï¼‰ï¼possibleï¼ˆå½¢å®¹è©ï¼šã‚ã‚Šå¾—ã‚‹ï¼‰ã§ã€likely ã¯ã€Œèµ·ã“ã‚Šãã†ï¼ˆç¢ºç‡ãŒé«˜ã‚ï¼‰ã€ã€possible ã¯ã€Œèµ·ã“ã‚Šå¾—ã‚‹ï¼ˆå¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰ã€ã¨ã„ã†å·®ãŒå‡ºã¾ã™ã€‚\\nã€å‚è€ƒã€‘be likely to doï¼ˆï½ã—ãã†ã ï¼‰ï¼It is likely that S+Vï¼ˆSãŒï½ã—ãã†ã ï¼‰\\nä¾‹ï¼šHe is likely to win. (å½¼ã¯å‹ã¡ãã†ã ã€‚)ï¼It is likely that prices will rise. (ç‰©ä¾¡ã¯ä¸ŠãŒã‚Šãã†ã ã€‚)"
```

**å‡ºåŠ›ä¾‹3ï¼ˆdue to / because ofï¼‰:**
```
"reason": "3æ–‡ç›®: He succeeded due to his careful planning.\\nï¼ˆå½¼ã¯ç¶¿å¯†ãªè¨ˆç”»ã®ãŠã‹ã’ã§æˆåŠŸã—ãŸã€‚ï¼‰\\ndue toï¼ˆå‰ç½®è©å¥ï¼šï½ãŒåŸå› ã§ï¼‰ï¼because ofï¼ˆå‰ç½®è©å¥ï¼šï½ã®ãŸã‚ã«ï¼‰ã§ã€ã©ã¡ã‚‰ã‚‚åŸå› ã‚’è¡¨ã—ã¾ã™ãŒã€due to ã¯ã‚„ã‚„ç¡¬ã‚ã§ã€ŒåŸå› ãƒ»è¦å› ã€ã‚’è¿°ã¹ã‚‹æ–‡ã«åˆã„ã‚„ã™ã„ï¼ˆåè©ã¨ç›¸æ€§ãŒè‰¯ã„ï¼‰ä¸€æ–¹ã€because of ã¯ã‚ˆã‚Šä¸‡èƒ½ã§ä¼šè©±ã§ã‚‚è‡ªç„¶ã§ã™ã€‚\\nã€å‚è€ƒã€‘due to Aï¼ˆAãŒåŸå› ã§ï¼‰ï¼because of Aï¼ˆAã®ãŸã‚ã«ï¼‰\\nä¾‹ï¼šThe delay was due to the storm. (é…ã‚Œã¯åµãŒåŸå› ã ã£ãŸã€‚)ï¼We stayed home because of the storm. (åµã®ãŸã‚å®¶ã«ã„ãŸã€‚)"
```


=== ä½¿ç”¨ä¾‹ã¨GPTã¸ã®è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ===



================================================================================
=== AGENT INSTRUCTIONS (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡ç¤º) ===
================================================================================

# âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŒ‡ç¤ºï¼ˆã‚³ãƒ”ãƒšç”¨ï¼‰

ã‚ãªãŸã¯ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®è‹±è¨³æ·»å‰Šï¼ˆtranslation_simpleï¼‰ç³»ã®ä¸å…·åˆã‚’ä¿®æ­£ã™ã‚‹å®Ÿè£…æ‹…å½“ã§ã™ã€‚
P0â†’P1â†’P2ã®å„ªå…ˆåº¦ã§ã€å›å¸°ã‚’èµ·ã“ã•ãšã«ç›´ã—ã¦ãã ã•ã„ã€‚

---

## 0) ã¾ãšèª­ã‚€ã‚‚ã®ï¼ˆå¿…é ˆï¼‰

- **BUG_REPORT_20260129.md** - ä¸å…·åˆã®å†ç¾æ¡ä»¶ãƒ»åŸå› ãƒ»æœŸå¾…å‹•ä½œãƒ»æ¨å¥¨ä¿®æ­£
- **HOW_TO_SUBMIT_TO_GPT.md** - å…±æœ‰æ‰‹é †ï¼ˆä»Šå›ã¯"ä¿®æ­£å®Ÿè£…"ãªã®ã§å‚è€ƒç¨‹åº¦ï¼‰
- **GPT_SUBMISSION.txt** - å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã¨ãƒ¬ãƒãƒ¼ãƒˆ

---

## P0ï¼ˆæœ€å„ªå…ˆï¼‰æ–‡åˆ†å‰²ãŒéå‰°ã«ãªã‚‹å•é¡Œã‚’ä¿®æ­£

### ã‚´ãƒ¼ãƒ«ï¼ˆåˆæ ¼æ¡ä»¶ï¼‰

- æ—¥æœ¬èªåŸæ–‡ãŒ3æ–‡ãªã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼è‹±æ–‡ã‚‚3æ–‡ã¨ã—ã¦æ‰±ãˆã‚‹ã“ã¨ï¼ˆå°‘ãªãã¨ã‚‚ã€Œ6æ–‡ã«å¢—ãˆã‚‹ã€çŠ¶æ…‹ã‚’æ½°ã™ï¼‰
- å…·ä½“ä¾‹ï¼ˆBUG_REPORTè¨˜è¼‰ã®å…¥åŠ›ï¼‰ã§ **6åˆ†å‰²â†’3åˆ†å‰²ã«æ”¹å–„**ã™ã‚‹ã“ã¨

### ç›´ã™å¯¾è±¡ï¼ˆå„ªå…ˆé †ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `points_normalizer.py`
- `normalize_user_input()`
- `split_into_sentences()`

### ä¿®æ­£æ–¹é‡ï¼ˆä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å®Ÿè£…ï¼‰

#### 1. æ”¹è¡Œã¯æ–‡åŒºåˆ‡ã‚Šã«ã—ãªã„ï¼ˆå¥èª­ç‚¹ãŒãªã„æ”¹è¡Œã¯ã€å‰å¾Œã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§çµåˆã™ã‚‹ï¼‰

ç¾åœ¨ã€Œæ”¹è¡Œâ†’ãƒ”ãƒªã‚ªãƒ‰æŒ¿å…¥ã€ãªã©ã‚’ã—ã¦ã„ã‚‹ãªã‚‰æ’¤å»/ç„¡åŠ¹åŒ–ã—ã€åŸºæœ¬ã¯ `\n+` â†’ `" "` ã«æ­£è¦åŒ–ã€‚

```python
# âŒ ç¾åœ¨ï¼ˆé–“é•ã„ï¼‰
lines = text.split('\n')
for line in lines:
    if not line.endswith('.'):
        line += '.'  # â† ã“ã‚ŒãŒéå‰°åˆ†å‰²ã®åŸå› 

# âœ… ä¿®æ­£å¾Œ
text = text.replace('\n', ' ')  # æ”¹è¡Œã¯å˜ãªã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã«
```

#### 2. `split_into_sentences()` ã®åˆ†å‰²æ¡ä»¶ã‚’å³æ ¼åŒ–

ç¾åœ¨ã€å°æ–‡å­—é–‹å§‹ã‚‚è¨±å®¹ã—ã¦ã„ã‚‹æ­£è¦è¡¨ç¾ã‚’ã€åŸå‰‡ã¨ã—ã¦ **[.!?] + whitespace + (å¤§æ–‡å­— or " or ' or () ã®ã¨ãã ã‘åˆ†å‰²** ã«å¤‰æ›´ã™ã‚‹ã€‚

```python
# âŒ ç¾åœ¨ï¼ˆé–“é•ã„ï¼‰
r'([.!?])\s*(?=[A-Za-z0-9"\'\(])'  # å°æ–‡å­—ã‚‚åˆ†å‰²

# âœ… ä¿®æ­£å¾Œ
r'([.!?])\s+(?=[A-Z"\'\(])'  # å¤§æ–‡å­—ã®ã¿åˆ†å‰²
```

ã“ã‚Œã«ã‚ˆã‚Šã€`... people so ...` ã‚„ `... in. this ...` ã®ã‚ˆã†ãªå°æ–‡å­—é–‹å§‹ã‚’ã€Œæ–°æ–‡ã€ã¨èª¤èªã—ãªã„ã€‚

#### 3. ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã®ã‚¹ãƒšãƒ¼ã‚¹ä¸è¶³ï¼ˆ`survey.Japan` ç­‰ï¼‰ã‚’æ•‘ã†

`normalize_user_input()` ã§ã€Œå¥èª­ç‚¹ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãŒç„¡ã„ã€ã‚±ãƒ¼ã‚¹ã¯ã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ã—ã¦OKï¼ˆã“ã‚Œã¯æ­£ã—ã„ï¼‰ã€‚

ãŸã ã—ã€ãã®çµæœ `split_into_sentences()` ãŒéå‰°åˆ†å‰²ã—ãªã„ã‚ˆã†ã«ä¸Šã®å³æ ¼åˆ†å‰²ãƒ«ãƒ¼ãƒ«ã«åˆã‚ã›ã‚‹ã€‚

#### 4. ï¼ˆæ¨å¥¨ï¼‰æœŸå¾…æ–‡æ•°ï¼ˆæ—¥æœ¬èªåŸæ–‡ã®æ–‡æ•°ï¼‰ã«åˆã‚ã›ã‚‹æœ€çµ‚ã‚»ãƒ¼ãƒ•ãƒ†ã‚£

å¯èƒ½ãªã‚‰ã€å‡¦ç†ã®ã©ã“ã‹ã§ã€Œæ—¥æœ¬èªåŸæ–‡ã®æ–‡æ•°ã€ã‚’å–å¾—ã§ãã‚‹ãªã‚‰ã€
`splitçµæœ > expected` ã®ã¨ãã¯ **ãƒãƒ¼ã‚¸ã—ã¦ expected ã«åã‚ã‚‹**ã€‚

**ãƒãƒ¼ã‚¸å„ªå…ˆæ¡ä»¶ä¾‹ï¼š**
- æ¬¡ã®æ–­ç‰‡ãŒå°æ–‡å­—é–‹å§‹ï¼ˆ`so`/`and`/`but`/`this`/`that`/`because`/`if`/`when`/`while` ãªã©ï¼‰ãªã‚‰å‰ã¨çµåˆ
- å‰ã®æ–­ç‰‡ãŒæ¥µç«¯ã«çŸ­ã„ï¼ˆä¾‹ï¼š`According to a recent survey.` ã®ã‚ˆã†ãªå°å…¥ã ã‘ï¼‰å ´åˆã‚‚æ¬¡ã¨çµåˆå€™è£œ

â€»å®Ÿè£…ãŒé‡ã‘ã‚Œã°ã€ã¾ãšã¯(1)(2)(3)ã ã‘ã§OKã€‚ä»Šå›ã®è‡´å‘½å‚·ã¯ã€Œéå‰°åˆ†å‰²ã€ãªã®ã§ã€‚

### å¿…é ˆã®å›å¸°ãƒ†ã‚¹ãƒˆè¿½åŠ 

`test_sentence_split_debug.py` ã«åŠ ãˆã€æœ€ä½ã§ã‚‚ä»¥ä¸‹ã‚’ãƒ†ã‚¹ãƒˆã«å…¥ã‚Œã‚‹ï¼ˆæ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚å¯ï¼‰ï¼š

```python
# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã‚¹ãƒšãƒ¼ã‚¹ãªã—
assert split("I like apples.She likes oranges.") == 2æ–‡

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: æ”¹è¡Œã§é€”ä¸­åˆ‡ã‚Œ
assert split("I like apples\nbut she likes oranges.") == 1æ–‡ï¼ˆçµåˆï¼‰

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: å°æ–‡å­—é–‹å§‹
assert split("I like apples. so does she.") == 1æ–‡ï¼ˆéå‰°åˆ†å‰²ã—ãªã„ï¼‰
```

---

## P1ï¼ˆé«˜å„ªå…ˆï¼‰ã€Œæœªæå‡ºã€ãªã®ã«LLMãŒå‹æ‰‹ã«ç”Ÿæˆãƒ»ç„¡é–¢ä¿‚è§£èª¬ã™ã‚‹å•é¡Œ

### ã‚´ãƒ¼ãƒ«ï¼ˆåˆæ ¼æ¡ä»¶ï¼‰

- ã€Œæœªæå‡ºã€æ‰±ã„ã®æ–‡ã«ã¤ã„ã¦ã€LLMãŒæ¨¡ç¯„è‹±ä½œæ–‡ã‚’å‹æ‰‹ã«ç”Ÿæˆã—ãªã„
- ã€Œæœªæå‡ºã€æ‰±ã„ã®æ–‡ã«ã¤ã„ã¦ã€ç„¡é–¢ä¿‚èªå½™è§£èª¬ï¼ˆ`appropriate`/`suitable`ç­‰ï¼‰ã‚’å‡ºã•ãªã„

### ä¿®æ­£æ–¹é‡ï¼ˆãŠã™ã™ã‚ï¼šã‚¢ãƒ—ãƒªå´ã§æŠ‘æ­¢ï¼‰

**LLMã«"æœªæå‡ºæ–‡"ã‚’æ¸¡ã•ãªã„è¨­è¨ˆã«å¯„ã›ã‚‹ã®ãŒæœ€ã‚‚å®‰å…¨ã€‚**

ä¾‹ï¼šæ–‡å¯¾å¿œä»˜ã‘ã®æ®µéšã§ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼è‹±æ–‡ãŒå­˜åœ¨ã—ãªã„ã€ãªã‚‰ã€ãã®é …ç›®ã¯ **å›ºå®šãƒ†ãƒ³ãƒ—ãƒ¬å‡ºåŠ›ã§åŸ‹ã‚ã‚‹**ï¼ˆPythonå´ã§ï¼‰ã€‚

- `reason` ã«ã¯å¿…ãšæ—¥æœ¬èªè¨³ï¼ˆåŸæ–‡ï¼‰ã‚’å…¥ã‚Œã‚‹ï¼ˆP2ã«ã‚‚åŠ¹ãï¼‰

### ã‚‚ã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå´ã§å¯¾å¿œã™ã‚‹å ´åˆï¼ˆæœ€ä½é™ï¼‰

`prompts_translation_simple.py` ã«ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’æ˜æ–‡åŒ–ã—ã€å‡ºåŠ›ä¾‹ã‚‚è¿½åŠ ï¼š

```python
### æœªæå‡ºã®æ–‡ã®å‡¦ç†ã€é‡è¦ã€‘

åŸæ–‡ã«ã‚ã‚‹æ–‡ã ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè‹±è¨³ã‚’æå‡ºã—ã¦ã„ãªã„å ´åˆ:
- level: "âœ…"
- before: "(æœªæå‡º)"
- after: "(æœªæå‡º)"
- reason: "Næ–‡ç›®: ï¼ˆæœªæå‡ºï¼‰\nï¼ˆæ—¥æœ¬èªè¨³: [åŸæ–‡ã‚’ãã®ã¾ã¾è¨˜è¼‰]ï¼‰\nã“ã®æ–‡ã¯æå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
- æ¨¡ç¯„è§£ç­”ã‚’å‹æ‰‹ã«ç”Ÿæˆã—ãªã„
- ç„¡é–¢ä¿‚ãªèªå½™è§£èª¬ã‚’å‡ºã•ãªã„ï¼ˆã‚¼ãƒ­ã«ã™ã‚‹ï¼‰
```

---

## P2ï¼ˆä¸­å„ªå…ˆï¼‰æ—¥æœ¬èªè¨³ï¼ˆNæ–‡ç›®: â€¦ï¼‰ã®è¡¨ç¤ºæ¬ è½ã‚’ä¿®æ­£

### ã‚´ãƒ¼ãƒ«ï¼ˆåˆæ ¼æ¡ä»¶ï¼‰

- âœ…ã§ã‚‚âŒã§ã‚‚æœªæå‡ºã§ã‚‚ã€å…¨é …ç›®ã«å¿…ãš
  ```
  Næ–‡ç›®:ï¼ˆæ—¥æœ¬èªè¨³ ...ï¼‰
  ```
  ãŒå…¥ã‚‹ã“ã¨

### å®Ÿè£…ãƒã‚¤ãƒ³ãƒˆ

1. ã€Œæœªæå‡ºã€åˆ†å²ã ã‘æ—¥æœ¬èªè¨³ãŒå‡ºãªã„ã€ãªã©ã®ä¾‹å¤–åˆ†å²æ¼ã‚Œã‚’æ½°ã™
2. ã§ãã‚Œã° `reason` ã‚’çµ„ã¿ç«‹ã¦ã‚‹é–¢æ•°ã‚’1ã‹æ‰€ã«å¯„ã›ã¦ã€å¿…é ˆè¦ç´ ã‚’å¼·åˆ¶ã™ã‚‹ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬åŒ–ï¼‰

---

## ä»•ä¸Šã’ï¼ˆå¿…é ˆï¼‰

1. **å†ç¾ãƒ†ã‚¹ãƒˆ**: BUG_REPORTã®å…¥åŠ›ã§ã€åˆ†å‰²æ•°ãƒ»å‡ºåŠ›å½¢å¼ãŒæ”¹å–„ã—ãŸãƒ­ã‚°ã‚’æ®‹ã™
2. **å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: æ—¢å­˜ãƒ†ã‚¹ãƒˆ + è¿½åŠ ãƒ†ã‚¹ãƒˆãŒé€šã‚‹ã“ã¨
3. **å·®åˆ†æç¤º**: å¤‰æ›´ã—ãŸé–¢æ•°ã¯ã€Œç½®ãæ›ãˆå¯èƒ½ãªå®Œå…¨ã‚³ãƒ¼ãƒ‰ã€ã§æç¤ºï¼ˆéƒ¨åˆ†è²¼ã‚Šã§ã¯ãªãï¼‰
4. **ã‚³ãƒŸãƒƒãƒˆ**:
   ```
   fix: sentence splitting no longer over-splits
   fix: prevent hallucinations for missing sentences
   fix: always show Japanese translation in reason
   ```

---

## æœŸå¾…ã™ã‚‹æœ€çµ‚ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆï¼ˆã“ã®é †ã§è¿”ã—ã¦ï¼‰

1. **ä¿®æ­£å†…å®¹ã®è¦ç´„**ï¼ˆP0/P1/P2ï¼‰
2. **å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§**
3. **ç½®æ›ç”¨ã®å®Œå…¨ã‚³ãƒ¼ãƒ‰**ï¼ˆè©²å½“é–¢æ•° or è©²å½“ãƒ•ã‚¡ã‚¤ãƒ«å·®åˆ†ï¼‰
4. **è¿½åŠ ã—ãŸãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰**
5. **BUG_REPORTä¾‹ã«å¯¾ã™ã‚‹ "æ”¹å–„å¾Œã®åˆ†å‰²çµæœ" ã®å…·ä½“è¡¨ç¤º**

---

## ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§

ã“ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š

1. **GPT_SUBMISSION.txt** - å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã¨ãƒã‚°ãƒ¬ãƒãƒ¼ãƒˆ
2. **BUG_REPORT_20260129.md** - è©³ç´°ãªä¸å…·åˆåˆ†æ
3. **test_sentence_split_debug.py** - ä¸å…·åˆã‚’å†ç¾ã™ã‚‹ãƒ†ã‚¹ãƒˆ
4. **points_normalizer.py** - ä¿®æ­£å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
5. **constraint_validator.py** - é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«
6. **prompts_translation_simple.py** - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©

---

## ğŸ¯ ä½¿ã„æ–¹

1. ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼
2. ChatGPT / Claude ã«è²¼ã‚Šä»˜ã‘
3. ã€ŒGPT_SUBMISSION.txt ã®å†…å®¹ã‚‚ç¢ºèªã—ã¦ãã ã•ã„ã€ã¨è¿½è¨˜
4. ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã‚’å—ã‘å–ã‚‹
5. ãƒ†ã‚¹ãƒˆã—ã¦ç¢ºèª
6. ã‚³ãƒŸãƒƒãƒˆ

---

**ä½œæˆæ—¥**: 2026å¹´1æœˆ29æ—¥
**å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: miyazaki_igaku_100wordsï¼ˆå®®å´å¤§å­¦åŒ»å­¦éƒ¨è‹±ä½œæ–‡ç‰¹è¨“ã‚·ã‚¹ãƒ†ãƒ ï¼‰
