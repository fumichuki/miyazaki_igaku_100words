"""
æ·»å‰Šãƒã‚¤ãƒ³ãƒˆã®æ­£è¦åŒ–å‡¦ç†
- æ–­ç‰‡ã‚’å…¨æ–‡ã«æ‹¡å¼µ
- level ã‚’ âŒ ã¾ãŸã¯ âœ… ã«å¼·åˆ¶
- âœ… ã®å ´åˆã¯ after=before ã«çŸ¯æ­£
- sentence_no ã‚’ä»˜ä¸
"""
import logging
import re
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


# çœç•¥å½¢ãƒªã‚¹ãƒˆï¼ˆãƒ”ãƒªã‚ªãƒ‰ã‚’å«ã‚€ãŒæ–‡æœ«ã§ã¯ãªã„ã‚‚ã®ï¼‰
_ABBREVIATIONS = [
    "a.m.", "p.m.", "e.g.", "i.e.", "etc.",
    "Mr.", "Mrs.", "Ms.", "Dr.", "Prof.",
    "U.S.", "U.K.", "vs.", "vol.", "fig.",
    "Jan.", "Feb.", "Mar.", "Apr.", "Aug.", "Sep.", "Oct.", "Nov.", "Dec.",
    "Mon.", "Tue.", "Wed.", "Thu.", "Fri.", "Sat.", "Sun."
]
_DOT_PLACEHOLDER = "<DOT>"


def _protect_abbreviations(text: str) -> str:
    """
    çœç•¥å½¢ã®ãƒ”ãƒªã‚ªãƒ‰ã‚’ä¸€æ™‚çš„ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ç½®æ›ã—ã¦ä¿è­·ã™ã‚‹
    
    æ–‡æœ«ã®çœç•¥å½¢ï¼ˆä¾‹: "U.S. It"ï¼‰ã®å ´åˆã€çœç•¥å½¢å†…éƒ¨ã®ãƒ”ãƒªã‚ªãƒ‰ã®ã¿ä¿è­·ã—ã€
    æ–‡æœ«ã®ãƒ”ãƒªã‚ªãƒ‰ã¯ä¿è­·ã—ãªã„
    
    Args:
        text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        ä¿è­·ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    protected = text
    
    # çœç•¥å½¢ã‚’ä¿è­·ï¼ˆãŸã ã—ã€æ–‡æœ«åˆ¤å®šã®ãŸã‚ç‰¹åˆ¥ãªå‡¦ç†ãŒå¿…è¦ï¼‰
    for abbr in _ABBREVIATIONS:
        # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«ãƒãƒƒãƒãƒ³ã‚°
        # ãŸã ã—ã€çœç•¥å½¢ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ãŒç¶šãå ´åˆã¯æ–‡ã®åŒºåˆ‡ã‚Šã¨è¦‹ãªã™
        # ä¾‹: "U.S. It" ã®å ´åˆã€"U.S." å…¨ä½“ã§ã¯ãªã "U.S" ã®ã¿ä¿è­·
        pattern = re.compile(re.escape(abbr), re.IGNORECASE)
        
        # çœç•¥å½¢ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ãŒç¶šãå ´åˆã¯ã€æœ€å¾Œã®ãƒ”ãƒªã‚ªãƒ‰ä»¥å¤–ã‚’ä¿è­·
        # ä¾‹: "U.S." â†’ "U<DOT>S."
        if abbr.endswith('.'):
            abbr_without_last_dot = abbr[:-1]  # "U.S." â†’ "U.S"
            # "U.S." ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹+å¤§æ–‡å­—ãŒç¶šãå ´åˆã®ã¿ã€æœ€å¾Œã®ãƒ”ãƒªã‚ªãƒ‰ã‚’æ®‹ã™
            protected = re.sub(
                re.escape(abbr) + r'(?=\s+[A-Z])',
                abbr_without_last_dot.replace(".", _DOT_PLACEHOLDER) + ".",
                protected,
                flags=re.IGNORECASE
            )
            # ãã‚Œä»¥å¤–ã®å ´åˆã¯å…¨ä½“ã‚’ä¿è­·
            protected = pattern.sub(
                lambda m: m.group(0).replace(".", _DOT_PLACEHOLDER),
                protected
            )
    
    # ã‚¤ãƒ‹ã‚·ãƒ£ãƒ«å½¢å¼ï¼ˆA.B.C.ãªã©ï¼‰ã‚’ä¿è­·
    protected = re.sub(r'\b([A-Z])\.(?=\s*[A-Z]\.)', r'\1' + _DOT_PLACEHOLDER, protected)
    
    # å°æ•°ï¼ˆ3.14ãªã©ï¼‰ã‚’ä¿è­·
    protected = re.sub(r'(\d)\.(\d)', r'\1' + _DOT_PLACEHOLDER + r'\2', protected)
    
    return protected


def _restore_abbreviations(text: str) -> str:
    """
    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ãƒ”ãƒªã‚ªãƒ‰ã«æˆ»ã™
    
    Args:
        text: ä¿è­·ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        å¾©å…ƒã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    return text.replace(_DOT_PLACEHOLDER, ".")


def normalize_user_input(text: str) -> str:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’æ­£è¦åŒ–ã™ã‚‹
    
    ä»¥ä¸‹ã®ä¿®æ­£ã‚’è¡Œã†ï¼š
    - å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
    - æ”¹è¡Œã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›ï¼ˆå¥èª­ç‚¹ãŒãªã„æ”¹è¡Œã¯æ–‡ã®é€”ä¸­ã¨ã—ã¦æ‰±ã†ï¼‰
    - ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãªãæ–‡å­—ãŒç¶šãå ´åˆã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥ï¼ˆä¾‹: "word.In" â†’ "word. In"ï¼‰
    - è¤‡æ•°ã®é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
    - æ–‡æœ«å¥èª­ç‚¹ã®å‰ã®ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
    - æ–‡æœ«ã«ãƒ”ãƒªã‚ªãƒ‰ãŒãªã„å ´åˆã¯è¿½åŠ 
    - å„æ–‡ã®æ–‡é ­ã‚’å¤§æ–‡å­—åŒ–ï¼ˆè‡ªå‹•æ•´å½¢ï¼‰
    - å‰å¾Œã®ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
    
    æ³¨æ„: ã‚¹ãƒšãƒ«ãƒŸã‚¹ãƒ»æ–‡æ³•ãƒŸã‚¹ãƒ»èªå½™ãƒŸã‚¹ã¯ä¿®æ­£ã—ãªã„ï¼ˆæ·»å‰Šå¯¾è±¡ã¨ã—ã¦æ®‹ã™ï¼‰
    
    Args:
        text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸè‹±æ–‡
    
    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸè‹±æ–‡
    """
    if not text or not text.strip():
        return ""
    
    # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    normalized = text.strip()
    
    # ã‚¹ãƒ†ãƒƒãƒ—0a: å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›ï¼ˆæœ€å„ªå…ˆï¼‰
    normalized = normalized.replace('ã€€', ' ')
    
    # ã‚¹ãƒ†ãƒƒãƒ—0b: æ”¹è¡Œã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
    # æ”¹è¡Œã¯æ–‡ã®åŒºåˆ‡ã‚Šã§ã¯ãªãã€å…¥åŠ›ã®é€”ä¸­ã¨è¦‹ãªã™ï¼ˆéå‰°åˆ†å‰²ã‚’é˜²ãï¼‰
    # ä¾‹: "...full by people\nso a lot..." â†’ "...full by people so a lot..."
    normalized = re.sub(r'\n+', ' ', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ”ãƒªã‚ªãƒ‰ç›´å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ãªãæ–‡å­—ãŒç¶šãå ´åˆã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŒ¿å…¥
    # çœç•¥å½¢ã‚’ä¿è­·ã™ã‚‹å‰ã«å®Ÿè¡Œï¼ˆp.m.In â†’ p.m. Inï¼‰
    # æ³¨: å¤§æ–‡å­—ãƒ»å°æ–‡å­—ä¸¡æ–¹ã«å¯¾å¿œï¼ˆsurvey.Japan â†’ survey. Japanï¼‰
    normalized = re.sub(r'\.([A-Za-z])', r'. \1', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ç–‘å•ç¬¦ãƒ»æ„Ÿå˜†ç¬¦ã®ç›´å¾Œã‚‚åŒæ§˜
    normalized = re.sub(r'([?!])([A-Za-z])', r'\1 \2', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: è¤‡æ•°ã®é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3.5: æ–‡æœ«å¥èª­ç‚¹ã®å‰ã®ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ï¼ˆä¾‹: "word ." â†’ "word."ï¼‰
    normalized = re.sub(r'\s+([.?!])$', r'\1', normalized)
    # æ–‡ä¸­ã®å¥èª­ç‚¹ã®å‰ã®ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚‚å‰Šé™¤ï¼ˆä¾‹: "word . Next" â†’ "word. Next"ï¼‰
    normalized = re.sub(r'\s+([.?!])\s+', r'\1 ', normalized)
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: æ–‡æœ«ã«ãƒ”ãƒªã‚ªãƒ‰ãƒ»ç–‘å•ç¬¦ãƒ»æ„Ÿå˜†ç¬¦ãŒãªã„å ´åˆã¯ã€ãƒ”ãƒªã‚ªãƒ‰ã‚’è¿½åŠ 
    if not normalized.endswith(('.', '?', '!')):
        normalized = normalized + '.'
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: å„æ–‡ã®æ–‡é ­ã‚’å¤§æ–‡å­—åŒ–ï¼ˆè‡ªå‹•æ•´å½¢ï¼‰
    # æœ€åˆã®æ–‡å­—ã‚’å¤§æ–‡å­—åŒ–
    if normalized:
        normalized = normalized[0].upper() + normalized[1:]
    
    # å¥èª­ç‚¹ï¼ˆ. ! ?ï¼‰ã®å¾Œã«ç¶šãæ–‡å­—ã‚’å¤§æ–‡å­—åŒ–
    # ä¾‹: "hello. the dog" â†’ "hello. The dog"
    # æ³¨: çœç•¥å½¢ï¼ˆe.g., i.e., p.m.ï¼‰ã®ç›´å¾Œã¯å¤§æ–‡å­—åŒ–ã—ãªã„
    def capitalize_after_punctuation(match):
        punctuation = match.group(1)  # . or ! or ?
        space = match.group(2)        # ã‚¹ãƒšãƒ¼ã‚¹
        letter = match.group(3)       # æ–‡å­—
        return punctuation + space + letter.upper()
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³: [.!?] + ã‚¹ãƒšãƒ¼ã‚¹ + å°æ–‡å­—
    # ä¾‹: ". the" â†’ ". The"
    normalized = re.sub(r'([.!?])(\s+)([a-z])', capitalize_after_punctuation, normalized)
    
    return normalized.strip()


def split_into_sentences(text: str) -> List[str]:
    """
    è‹±æ–‡ã‚’ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã«åˆ†å‰²ã™ã‚‹ï¼ˆçœç•¥å½¢ã«å¯¾å¿œãƒ»å³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
    
    p.m., a.m., e.g., U.S. ãªã©ã®çœç•¥å½¢ã®ãƒ”ãƒªã‚ªãƒ‰ã§åˆ†å‰²ã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹
    
    åˆ†å‰²æ¡ä»¶ï¼ˆå³æ ¼åŒ–ï¼‰:
    - ãƒ”ãƒªã‚ªãƒ‰ãƒ»ç–‘å•ç¬¦ãƒ»æ„Ÿå˜†ç¬¦ + ã‚¹ãƒšãƒ¼ã‚¹ + å¤§æ–‡å­—/å¼•ç”¨ç¬¦/æ‹¬å¼§ã®ã¿
    - å°æ–‡å­—å§‹ã¾ã‚Šã¯å‰ã®æ–‡ã®ç¶™ç¶šã¨è¦‹ãªã™ï¼ˆéå‰°åˆ†å‰²ã‚’é˜²ãï¼‰
    
    Args:
        text: è‹±æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    if not text or not text.strip():
        return []
    
    # æ”¹è¡Œã¯ normalize_user_input() ã§æ—¢ã«ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›æ¸ˆã¿
    # ã“ã“ã§ã¯æ”¹è¡Œãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: çœç•¥å½¢ã®ãƒ”ãƒªã‚ªãƒ‰ã‚’ä¿è­·
    protected = _protect_abbreviations(text.strip())
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: æ–‡æœ«å€™è£œã®ãƒ”ãƒªã‚ªãƒ‰ã‚’æ¤œå‡ºï¼ˆå³æ ¼åŒ–ï¼‰
    # æ¡ä»¶: [.!?] + ã‚¹ãƒšãƒ¼ã‚¹1ã¤ä»¥ä¸Š + (å¤§æ–‡å­— or å¼•ç”¨ç¬¦ or æ‹¬å¼§)
    # å°æ–‡å­—å§‹ã¾ã‚Šï¼ˆso, this, that ãªã©ï¼‰ã¯æ–°ã—ã„æ–‡ã¨ã—ã¦æ‰±ã‚ãªã„
    parts = re.split(r'([.!?])\s+(?=[A-Z"\'\(])', protected)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†å‰²çµæœã‚’æ–‡ã«å†æ§‹æˆ
    sentences = []
    i = 0
    while i < len(parts):
        if i + 1 < len(parts) and parts[i + 1] in '.!?':
            # ãƒ†ã‚­ã‚¹ãƒˆ + å¥èª­ç‚¹ã‚’çµåˆ
            sentence = parts[i] + parts[i + 1]
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å¾©å…ƒ
            restored = _restore_abbreviations(sentence)
            # é‡è¤‡ãƒ”ãƒªã‚ªãƒ‰ã‚’å‰Šé™¤ï¼ˆU.S.. â†’ U.S.ï¼‰
            restored = re.sub(r'\.\.+', '.', restored)
            sentences.append(restored)
            i += 2
        else:
            # æœ€å¾Œã®éƒ¨åˆ†ï¼ˆå¥èª­ç‚¹ãªã—ï¼‰
            if parts[i].strip():
                restored = _restore_abbreviations(parts[i])
                sentences.append(restored)
            i += 1
    
    # ç©ºã®è¦ç´ ã‚’å‰Šé™¤ã—ã€ä¸¡ç«¯ã®ç©ºç™½ã‚’å‰Šé™¤
    sentences = [s.strip() for s in sentences if s.strip()]
    
    return sentences


def find_sentence_containing_fragment(fragment: str, sentences: List[str]) -> tuple:
    """
    æ–­ç‰‡ã‚’å«ã‚€ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’æ¢ã™
    
    Args:
        fragment: æ–­ç‰‡ãƒ†ã‚­ã‚¹ãƒˆ
        sentences: ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã®ãƒªã‚¹ãƒˆ
    
    Returns:
        (sentence_index, sentence_text) ã¾ãŸã¯ (None, None)
    """
    fragment_lower = fragment.lower().strip()
    
    for i, sentence in enumerate(sentences):
        if fragment_lower in sentence.lower():
            return (i, sentence)
    
    return (None, None)


def replace_fragment_in_sentence(sentence: str, before_fragment: str, after_fragment: str) -> str:
    """
    ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å†…ã®æ–­ç‰‡ã‚’ç½®æ›ã™ã‚‹ï¼ˆ1å›ã®ã¿ï¼‰
    
    Args:
        sentence: å…ƒã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹
        before_fragment: ç½®æ›å‰ã®æ–­ç‰‡
        after_fragment: ç½®æ›å¾Œã®æ–­ç‰‡
    
    Returns:
        ç½®æ›å¾Œã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹
    """
    # å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã›ãšã«1å›ã ã‘ç½®æ›
    pattern = re.compile(re.escape(before_fragment), re.IGNORECASE)
    result = pattern.sub(after_fragment, sentence, count=1)
    return result


def normalize_level(level: str, before: str, after: str) -> tuple:
    """
    level ã‚’ âŒ ã¾ãŸã¯ âœ… ã«æ­£è¦åŒ–ã—ã€after ã‚’èª¿æ•´ã™ã‚‹
    
    ãƒ«ãƒ¼ãƒ«:
    - ğŸ’¡ ãŒå«ã¾ã‚Œã‚‹ â†’ âœ… ã«å¤‰æ›ã—ã€after=before
    - level ãŒç„¡ã„ â†’ âœ… ã«å¤‰æ›ã—ã€after=before
    - âŒ ã®ã¨ãã¯ beforeâ‰ after ã‚’è¨±å¯
    - âœ… ã®ã¨ãã¯ after=before ã«çŸ¯æ­£
    
    Args:
        level: å…ƒã® level
        before: ä¿®æ­£å‰ã®è‹±æ–‡ï¼ˆå…¨æ–‡ï¼‰
        after: ä¿®æ­£å¾Œã®è‹±æ–‡ï¼ˆå…¨æ–‡ï¼‰
    
    Returns:
        (normalized_level, normalized_after)
    """
    # level ãŒç„¡ã„ã€ã¾ãŸã¯ ğŸ’¡ ã‚’å«ã‚€å ´åˆ
    if not level or 'ğŸ’¡' in level:
        logger.info(f"Normalizing level: '{level}' â†’ 'âœ… æ­£ã—ã„è¡¨ç¾' (after=before)")
        return ('âœ… æ­£ã—ã„è¡¨ç¾', before)
    
    # âŒ ã®å ´åˆã¯ãã®ã¾ã¾
    if 'âŒ' in level:
        logger.info(f"Level is âŒ, keeping after: '{after[:50]}...'")
        return (level, after)
    
    # âœ… ã®å ´åˆã¯ after=before ã«çŸ¯æ­£
    if 'âœ…' in level:
        if before != after:
            logger.info(f"Level is âœ… but afterâ‰ before. Setting after=before")
        return (level, before)
    
    # ãã®ä»–ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ âœ…
    logger.info(f"Unknown level '{level}', defaulting to 'âœ… æ­£ã—ã„è¡¨ç¾' (after=before)")
    return ('âœ… æ­£ã—ã„è¡¨ç¾', before)


def normalize_points(
    points: List[Dict[str, Any]],
    normalized_answer: str,
    japanese_sentences: List[str],
    original_user_answer: str = None
) -> List[Dict[str, Any]]:
    """
    points ã‚’æ­£è¦åŒ–ã™ã‚‹
    
    1. before/after ã‚’å…¨æ–‡ã«æ‹¡å¼µ
    2. level ã‚’ âŒ ã¾ãŸã¯ âœ… ã«å¼·åˆ¶
    3. âœ… ã®å ´åˆã¯ after=before ã«çŸ¯æ­£
    4. sentence_no ã‚’ä»˜ä¸
    5. sentence_no æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
    6. original_before ã‚’è¿½åŠ ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è¡¨ç¤ºç”¨ï¼‰
    
    Args:
        points: LLMã‹ã‚‰è¿”ã•ã‚ŒãŸ points
        normalized_answer: æ­£è¦åŒ–ã•ã‚ŒãŸå­¦ç”Ÿè‹±æ–‡
        japanese_sentences: æ—¥æœ¬èªåŸæ–‡ã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ãƒªã‚¹ãƒˆ
        original_user_answer: æ­£è¦åŒ–å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        æ­£è¦åŒ–ã•ã‚ŒãŸ points
    """
    logger.info(f"Starting points normalization: {len(points)} points")
    
    # å­¦ç”Ÿè‹±æ–‡ã‚’ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã«åˆ†å‰²ï¼ˆæ­£è¦åŒ–å¾Œï¼‰
    student_sentences = split_into_sentences(normalized_answer)
    logger.info(f"Student answer split into {len(student_sentences)} sentences")
    
    # å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚‚ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã«åˆ†å‰²ï¼ˆæ­£è¦åŒ–å‰ï¼‰
    original_sentences = []
    if original_user_answer:
        # æ­£è¦åŒ–å‰ã®å…¥åŠ›ã‚’åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§åˆ†å‰²ï¼ˆãƒ”ãƒªã‚ªãƒ‰ãªã—ã§ã‚‚å¯¾å¿œï¼‰
        original_sentences = split_into_sentences(original_user_answer)
        logger.info(f"Original user input split into {len(original_sentences)} sentences")
    
    normalized_points = []
    
    for i, point in enumerate(points):
        try:
            original_before = point.get('before', '').strip()
            original_after = point.get('after', '').strip()
            original_level = point.get('level', '')
            
            logger.info(f"Processing point {i+1}: before='{original_before[:50]}...', level='{original_level}'")
            
            # before ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not original_before:
                logger.warning(f"Point {i+1}: Empty before, skipping")
                continue
            
            # ğŸš¨é‡è¦: LLMãŒè¿”ã™ before ã‚‚æ­£è¦åŒ–ã™ã‚‹ï¼ˆãƒ”ãƒªã‚ªãƒ«è£œå®Œãªã©ï¼‰
            normalized_before = normalize_user_input(original_before)
            logger.info(f"Point {i+1}: Normalized before='{normalized_before[:50]}...'")
            
            # æ–­ç‰‡ â†’ å…¨æ–‡ã«æ‹¡å¼µï¼ˆæ­£è¦åŒ–å¾Œã® before ã§æ¤œç´¢ï¼‰
            sentence_index, full_sentence = find_sentence_containing_fragment(normalized_before, student_sentences)
            
            if full_sentence is None:
                # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è­¦å‘Šã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                logger.warning(f"Point {i+1}: Fragment '{original_before[:50]}' not found in student answer, skipping")
                continue
            
            logger.info(f"Point {i+1}: Found in sentence {sentence_index + 1}: '{full_sentence[:50]}...'")
            
            # before ã‚’å…¨æ–‡ã«ç½®æ›ï¼ˆæ—¢ã«æ­£è¦åŒ–æ¸ˆã¿ã®æ–‡å­—åˆ—ã‚’ä½¿ç”¨ï¼‰
            full_before = full_sentence
            
            # after ã‚‚æ­£è¦åŒ–
            normalized_after = normalize_user_input(original_after)
            
            # after ã‚’å…¨æ–‡ã«æ‹¡å¼µï¼ˆnormalized_after ãŒæ–­ç‰‡ã®å ´åˆã€ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å†…ã§ç½®æ›ï¼‰
            if 'âŒ' in original_level and normalized_before != normalized_after:
                # ä¿®æ­£ãŒå¿…è¦ãªå ´åˆï¼šnormalized_before ã‚’ normalized_after ã«ç½®æ›
                full_after = replace_fragment_in_sentence(full_sentence, normalized_before, normalized_after)
                # ä¿®æ­£å¾Œã®æ–‡å­—åˆ—ã‚‚æ­£è¦åŒ–ï¼ˆå¿µã®ãŸã‚ï¼‰
                full_after = normalize_user_input(full_after)
                logger.info(f"Point {i+1}: Replaced fragment in sentence: '{full_after[:50]}...'")
            else:
                # ä¿®æ­£ä¸è¦ãªå ´åˆï¼šafter ã¯ before ã¨åŒã˜
                full_after = full_before
            
            # level ã‚’æ­£è¦åŒ–ã—ã€å¿…è¦ãªã‚‰ after ã‚’èª¿æ•´
            normalized_level, final_after = normalize_level(original_level, full_before, full_after)
            
            # å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼ˆæ­£è¦åŒ–å‰ï¼‰ã‚’å–å¾—
            original_before_text = full_before  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ­£è¦åŒ–å¾Œ
            if original_sentences and sentence_index < len(original_sentences):
                original_before_text = original_sentences[sentence_index]
                logger.info(f"Point {i+1}: Original user input: '{original_before_text[:50]}...'")
            
            # sentence_no ã‚’ä»˜ä¸
            # japanese_sentence ãŒã‚ã‚Œã°ãã‚Œã‚’å…ƒã«ç‰¹å®šã€ãªã‘ã‚Œã° sentence_index+1
            sentence_no = sentence_index + 1
            if point.get('japanese_sentence'):
                # æ—¥æœ¬èªåŸæ–‡ã‹ã‚‰ index ã‚’ç‰¹å®šï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                try:
                    jp_index = japanese_sentences.index(point['japanese_sentence'])
                    sentence_no = jp_index + 1
                    logger.info(f"Point {i+1}: Matched Japanese sentence, sentence_no={sentence_no}")
                except ValueError:
                    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ sentence_index+1 ã‚’ä½¿ç”¨
                    logger.warning(f"Point {i+1}: Japanese sentence not found in original, using sentence_index+1")
            
            # æ­£è¦åŒ–çµæœã‚’è¨­å®š
            point['before'] = full_before
            point['after'] = final_after
            point['level'] = normalized_level
            point['sentence_no'] = sentence_no
            point['original_before'] = original_before_text  # æ­£è¦åŒ–å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            
            normalized_points.append(point)
            logger.info(f"Point {i+1}: Normalized successfully (sentence_no={sentence_no})")
        
        except Exception as e:
            logger.error(f"Error normalizing point {i+1}: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue
    
    # sentence_no æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
    normalized_points.sort(key=lambda p: p.get('sentence_no', 9999))
    
    logger.info(f"Points normalization complete: {len(normalized_points)} points")
    return normalized_points
